Since the methodology used has a bearing on the success of the project in hands. It is of high importance to choose which
practices should be adopted to effectively move forward. By means of this, investigation under which arsenal were to use is
counseled. After a lot of digging, it was decided that the number of different practices should not be to vast, otherwise it
would be a burden to put them to work together seamlessly.

Having a research methodology is full of advantages like: helping plan, structure and conduct
the research in a way which is structured and objective.

Research methodologies can largely be categorized into quantitative, qualitative and mixed.
Quantitative focus on gathering numerical data and performing statistical analysis to draw conclusions. Qualitative
on the other hand, is more suited to specify detailed insights such as behavior,motivations and experiences, which means that is more
theoretical while the quantitative is more empirical. Finally, there is also the concept of both which stands for having
both quantitative and qualitative.

During this thesis, in terms of research methodologies, there will only be space for two practises:  DSR(Design Science Research),
which in terms of nature of findings is hybrid, thereby categorized as a both approach like mentioned in the last paragraph and
SWOT which is more qualitative. The first one is more complete and more suited for the projects while the second will
be used more to drive the conclusions of the project to further analysis of it's pros and cons of usage of the project in cause. With
all of these settled, lets move to deeper thoughts about each.

%
%
%    DSR
%
%
\subsection{Design Science Research (DSR)}
As an important gadget for the project, DSR was the embraced tool. Considering it's traits, resembling extreme focus
on solving real problems, it represents indeed a powerful tool for going on with this thesis. Some people gain knowledge through
reading while other's achieve so by earring,practising or even speaking with other's. It's a serious challenge understand how a
team will work best but,due to the nature of the team, DSR proved to be the best choice for it's phenomenal practise and
rebuild model. In a distinct set of terms, this methodology is notably better for individuals which praise more the practical way of
things and by it's landing into the creation of artifacts for solving the current landscape of problems, creating this way solutions
and still gaining theoretic knowledge, it does outpace other methodologies making it better for the expected aspiration of
this project \cite{dsr-book}.

\subsubsection{Goals of DSR}
For the verbal reference of the DSR goals, there is a colossal number of concepts that could be spotted as
one. The gross of it, outcomes from the requirement of appeasing certain fields such as information systems,
engineering and others. This project is within the information systems realm, which means that these values are more than fit
for the project and thereby qualified to be mentioned here. By means of this, the most captious goals will be pronounced here.

At the heart, solving practical problems is one of those goals. This is reached by the design and creation of such
research artifacts that are implemented into practise, where those are seen as the form of models,frameworks,methods,processes,systems
and even tools. With accordance to this means, practical problems are solved and theoretical knowledge comes as an extension.

Besides that, there is also the goal of creating innovative artifacts. It's presence is requested when it comes to push the
horizon of existing knowledge and capabilities. As Innovation takes precedence in any kind of project, it could not be different when
it comes to the realization of this research, standing as a must for the DSR.

Tertiary to this, surges the contributing to Scientific Knowledge. Which is conventional, if further thinking is done through
the definition and name of the methodology. Knowledge occurs by extension when depared with the solving resolve. Thus, the more knowledge
extended bigger the contribution and new perspectives and ideas may flow, creating an avalanche of future projects, instigating development
and furthermore improving sectors.

Onward to the next objective, there can be Evaluation and Validation, since the evaluation itself is a pillar for the loop
created within the DSR realm and so it is validation but, although valid, it does not mean that it should be final. On this
assumptions, rigid considerations must be accommodated, however, solutions keep maturing which explains the continuous satisfaction
of this precise objective therefore nothing remains fully complete, which is expected from the point view of innovation.

Another objective important to consider is Balancing Rigor and Relevance, where it stands glamorously within DSR to
make sure that nothing becomes to much theoretical until the point there is no practical implementation. This is a risk in the
research world, where by applying to much importance to the theory category, the project start loosing color into it's true serving
purpose. With this in aim, exists there the importance of instituting a balance between the two dimensions, emphasing both the theoretical
inception but also the practical. This becomes the best approach, since the practical determines the verocity of the empirical sphere.

Lastly, there is the existence of another aspiration, denoted by Generalizability of Solutions. It's priority resides within the
sphere of making sure that, knowledge produced within the project can be adopted into broader contexts, from a deck of various industries
or scenarios. This bears a huge relevance, because by having countless scenarios of usage,new use cases are produced, thereby relevance of
the knowledge becomes enormous and so it's contribution for the research world, which affects the world by extension,proportionally to the desire of the
research itself.

\subsubsection{The DSR Process}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/tools/dsr/DSR.drawio.png} % Change this to your image file
    \caption{DSR Life-cycle}
    \label{fig:sample-image}
\end{figure}
Entering into the process of the DSR realm, there must be an understanding that this is a structured and iterative methodology,
focused on construction,evaluation and improvement of artifacts. The process is extensive, but it covers all the steps necessary
from problem identification to evaluation. Since it works around incrementation, this process is within a loop processing always new forms
of solutions and knowledge, since it produces both \cite{dsr-book}.

Problem Identification and Motivation refers to the first step. Here, there is a preoccupation into picturing a real-world
problem which seeks solution. To glean such information, various perspectives must be attended such as the current state,context and
impact on the project under scrutiny. The spotted problem should be doted of meaning and with a given degree of relevance. The objective
here is to gain power to articulate the problem clearly and make sure the team has the motivation to solve it. A good example to
achieve this is in healthcare, where there could be observed a problem with inefficiencies in patient management.

As a second step, there is Define Objectives of the Solution. After problem identification, it is of huge gravity to think
in a given set of objectives for the future solution. The objectives could be qualitative or quantitative, even thought the quantitative
are better, establishing them in the beginning is hard. Sometimes it is not possible and there are also situations where requirements
already give them away. The objective in this step is to set expected outcomes and conduct obligations for the artifact that will be
forged. An example for this would be setting an objective, creating a system that should be capable of reach 50 request per second.
But it could be a more simpler thing like giving good data for decision making in a healthcare facility.

Design and Development of the Artifact surges as the third step. When landed in this step, there is work to develop
the artifact (model,method,system or framework) that will be managed to meet the objectives created on the second step. With
the problem and objectives on hand, the team must create,conceptualize and refine it's artifact to better meet the expectations. In another
words, the objective is create a artifact that addresses these problems. Developing a prototype of a healthcare data management
system can be a good example of what is expected in this phase.

Moving to the forth step Demonstration, here the artifact is assigned to work, by either creating a simulation or
simply putting it in a real world system to check if it does meet the expectations. This stage is critical, because it serves as
proof of concept to inquiry if it has utility or not. Regardless of the result, knowledge is generated and the artifact has
room to grow because of the feedback created in this phase. The objective is to demonstrate if the problem can be solved or not with this
implementation by a practical setting. One example of this would be the deployment of a healthcare management system.

Evaluation on another hand, corresponds to the fifth step, where the evaluation of the artifact takes place. In this phase,
the product is evaluated. Rigor,effectiveness,efficiency and overall impact are considered, taking into consideration the problem and that
the evaluation method may defer relatively to the nature of the given product. This step has the objective of measuring if the
artifact solves the problem and meets the objectives settled in the earlier stages. As an example, the
evaluation of the performance by tracking the key metrics can be considered.

At the realm of the sixth step, Iteration and Refinement is the step right after the evaluation. This is because, based
on the observations, there may be areas where the artifact still lacks which compel for improvement. With this in mind, iteration
is done and evaluation and demonstration occur multiple times until there starts to have insignificant improvements. As an objective, in
this phase, there is the max refinement possible of the artifact recurring to iteration. To illustrate this, there is the
possibility of imagining a case where after testing, the system might be updated to include additional data fields or enhanced user
interface features to improve usability.

Finally, in the sept and last step there is the Communication phase. Within this, the research findings are shared, artifact
design and theoretical contributions are released and the form it takes are academic papers,presentations and
reports to stakeholders. Deconstructing this, there is the documentation and dissemination of the findings, including both
practical and theoretical contributions. As an example, there is the Publishment of a paper describing some development in healthcare
made by a given research.

\subsubsection{Usage during this project}

The usage of this methodology will be more comprehensive during the second use case. This is because the first one is more theoretical
and the second is more practical. Since this approach goes around creating knowledge through a practical multiple loop iteration, it
does require that the project is dotted of such.

With this in mind and better discussed further, firstly, within this work there is a definition of objectives for this solution. Secondly,
there is the design and development of the artifact, which is a blockchain network. Thirdly, this network passes by a demonstration to
effectively see how it behaves. Forthly, evaluations around the solution are made, and in case the solution is already good enough,
communication is made; otherwise, there is once again a process of iteration and refinement that starts once again in the demonstration,
creating this way an infinite loop until the solution satisfies the project needs.

%
%
%    SWOT ANALYSIS
%
%
\subsection{SWOT analysis}
SWOT analysis is a strategic planning tool which is often related to business. This is because it is a simple and yet powerful
overview, that empowers organizations to set a plan for that precise moment. The acronymom SWOT, stands in English for
Strengths, Weaknesses, Opportunities and Threats, which represent the key factors that influence
the organizations success or failure. If done multiple times and by several people, it can gain multiple new perspectives that help as a
decision factor because it can probably give a deeper understanding of the company competitive position, identifying potential areas for
growth and also empower the staff to think better in strategies to address their challenges.
The purpose is to provide a well structured framework for decision making, which accesses internal factors like strengths and weaknesses
and also external factors like opportunities and threats.
Despite it's current usage being more directed to business, it also helps in other kind of projects such as personal career planning and
even research. In this project there's the intention to use it. This is because the use cases are complex, having a tool like this can
help others understand the current situation, can help the stakeholders of the research to work according to the books and it can give a
hint to those interested on the project to discover if it is doing well or not \cite{swot}.

\subsubsection{Internal Factors: Strengths and Weaknesses}
Representing the upper part of the SWOT standard visualization, there is the internal factors
Strengths and Weaknesses. This is what influences the most, the ability to the achievement of the
organization objectives. Understanding both strengths and weaknesses is crucial as a building block for enhancing existing
capabilities and check out areas where the organization usually outperforms or the opposite. To put in other terms, the organization
has the capability for control and actively manage this two, in a way where the focus is to improve the decisions to accomplish such.This
are not always deterministic into that direction but a deep dive into each will make this even more clear:

\paragraph{A. Strengths}\mbox{}\\
Strengths are the internal contributions, so that the organization has a certain number of attributes,resources and capabilities
for enterprise level competitive advantage. These are the areas or aspects where the company can outperform others, that can be used to
collect opportunities and even defend against possible threats. Identifying strengths is something very valuable for organizations, because
it helps in focusing on the best attributes of the organization, thereby fixing their market position. As an example of Strengths, it
could be something like: Strong brand Reputation, where a well established brand is widely recognized and trusted by
consumers. Skilled Workforce, in cases where there is a qualified and experienced team who drive innovation and efficiency.
Proprietary Technology, which are unique or patented technologies that provide a competitive edge in the
market. Operational Efficiency, for processes and effective cost management that lead to higher
profitability. Customer Loyalty, a loyal customer base that provides consistent revenue and positive word-of-mouth marketing
and finally, more concretely for research,Having more than enough resources, which stands for having such resources that makes
the research something more than feasible which usually is an huge challenge. By levering this strengths, an organization can maintain or
even enhance it's market position, thereby maximize its competitive advantages in relation to others, improving by extension its overall
economic performance.

\paragraph{B. Weaknesses}\mbox{}\\
Weaknesses are the core stone of the internal given limitations, for areas where the organization actually under-performs
relatively to competitors which by extension can prevent the organization from achieving it's objectives, by representing a obstacle to
the capitalization of the existing opportunities and even by exposing the company to risks presented in external threats. Weaknesses,
if spotted correctly, allow the organization to take measures for correctness and minimization of the impact that this unfortunate
characteristics may pose in future iterations. Addressing this is essential for vulnerabilities mitigation, improve performance and
prevent competitors from exploiting these causalities.
As an example, there is a lot of things that can be mentioned like: Outdated Technology, because by relying in older systems
or tech can slow down processes,limit capabilities and even possibly increase costs for maintenance. Limited Financial Resources,
where insufficient capital to invest can pose a problem to initiatives,R\&D and further expansion if desired. Inefficient Processes,
in the idea of resource wasting and higher costs resulted from activities and delays in delivery. Weak Brand Presence, which
represent a weakness in the essence of reduced market share and finally High Employee Turnover, where talent within the company
is hard to maintain, thereby resulting in higher recruitment and training costs.

\paragraph{C. Both combined}\mbox{}\\
By understanding and analysing Strengths and Weaknesses, there is the possibility of creating a good synergy between
both which can result in organizations positioning themselves to achieve their goals. Strengths help creating and sustaining
competitive advantages and Weaknesses ensures that there are no barriers for further success. In other words, considering both
interchangeably will imply future success, therefore it is very important to careful think in all of those and afterwards try to see it's
relations.


\subsubsection{External Factors: Opportunities and Threats}
When it comes to external factors, there should be informed that those factors are the ones that the enterprise cannot control, since
they don't own them and they are external to their direct manage. This means that despite the organization not being able to coordinate
them, they can still take actions to make sure that those causalities don't affect them that much or that it don't affect them in any way. These aspects are
Opportunities and Threats and they play a huge paper in the SWOT analysis. Putting in different terms, those are the existences that are based in outside
situations that are beyond their control, such as market trends, macro-economic indicators, competition and others. Opportunities provide a path for growth, while Threats present a
potential challenge that could negatively impact the company.
A deep dive into each factor will be done in this section:

\paragraph{A. Opportunities}\mbox{}\\
By identifying opportunities, organizations are doted of taking proactive steps toward expanding their market presence or even aggregating
more into their competitive deck of cards. To achieve this, the organizations must attend to their external circumstances in order to
effectively search over exploits that they can take into accordance to represent a new advantage. These, if used correctly, can help the
organization grow, innovate and improve performance.
As an example, there is the following possible aspects: Emerging Markets, which stands for entering into a new geographic or
demographic market, that offers potential for growth and expansion. Technological Advancements, that leverage new technologies
that improve efficiency, number of product offerings or new innovative services.Regulatory Changes, related to new policies or
regulatory shifts that if,taken into favor, can benefit substantially the organization in scope. Shifts in Consumer Preferences,
that aims to answer to new consumer and trend behavior that can be targeted by a new product that may be on release and finally
Strategic Partnerships, where forming alliances and collaborations with other companies can open new revenue streams.
By paying attention to opportunities, organizations can gain a competitive advantage, enter new markets and drive long-term growth.

\paragraph{B. Threats}\mbox{}\\
Threats on the other hand, are challenges or risks that could affect negatively the organization. Some of these are expected, but
the worst ones are those that no one is expecting and because of this there must be a lot of thoughts about it, because a lot of this comes
from disguised situations. Being able to spot such things, is a important skill when it comes to project maintainability and therefore it
is specially important to pay attention to, because it can be the meaning of the project ceasing and catching them early can help to take
preventive measures that could minimize or even remove the effects from it. Due to the external nature, the enterprise has low control over
this kind of situations but being interested in such matters, can reduce significantly the downturns.
As an example, threats can be represented as Increased Competition, where the increase of offers of similar products can
potentially remove some market share. Macro-economic downturns, where the economy slows down for some reason which results in
less demand for the project. Regulatory changes, which can result in the enterprise suffering by politics that may be against it
or that revoke previous given benefits. Changing Consumer Preferences, that happens when the consumer shifts it's interest in the
current product and benefits the competitor leaving the organization behind and Technological Disruptions where the advances in
technology actually remove the need that the enterprise was serving, therefore making the organization service/product obsolete.
In case well informed, organizations can adapt and overcome this challenges, by preparing defensive strategies that will mitigate
risks,ensure business continuity and market share protection.

\subsection{Risk List}

In the following table, the risks that directly or indirectly influenced the development of this dissertation are listed. Depending
on the identified risk, a mitigating action is proposed in order to minimize the impact of the risk. The scale for the Probability and
Impact columns is measured from 1 to 5, where 1 corresponds to a very low risk and 5 to a very high risk.

\begin{longtable}{|c|p{2cm}|c|c|c|p{2cm}|c|}

    \hline
     \textbf{Nº} &
     \textbf{Description} &
     \textbf{Probability(P)} &
     \textbf{Impact(I)} &
     \textbf{Seriety(P*I)} &
     \textbf{Mitigation Action} &
     \textbf{Verified}
     \\ \hline
    \endfirsthead

    \hline
    \textbf{1}
    &
    Unreachable deadlines
    & 3
    & 5
    & 15
    & Planning better the next events and gain experience from previous deliveries;
    & Yes
    \\ \hline

    \textbf{2}
    &
    Requirements of the project change
    & 2
    & 4
    & 8
    & Make frequent communication with stakeholder;Check well the limitations imposed by the project;
    & No
    \\ \hline

    \textbf{3}
    &
    Lack of Time
    & 4
    & 5
    & 20
    & Implement only what is actually needed
    & Yes
    \\ \hline

     \textbf{4}
    &
    Resources
    & 4
    & 5
    & 20
    & There are no mitigation measures
    & Yes
    \\ \hline

     \textbf{5}
    &
    Incorrect understanding of data
    & 2
    & 5
    & 10
    & Spend more time analyzing the data; Make sure that the data is relevant for the object of study;
    & Yes
    \\ \hline
    \end{longtable}



    \begin{longtable}{|c|p{2cm}|c|c|c|p{2cm}|c|}
        \hline
     \textbf{Nº} &
     \textbf{Description} &
     \textbf{Probability(P)} &
     \textbf{Impact(I)} &
     \textbf{Seriety(P*I)} &
     \textbf{Mitigation Action} &
     \textbf{Verified}
     \\ \hline
    \endfirsthead
    \textbf{6}
    &
    Mistakes
    & 1
    & 5
    & 5
    & Don't do tasks in automate mode; Focus more in the tasks;
    & Yes
    \\ \hline

    \textbf{6}
    &
    Security implications
    & 1
    & 5
    & 5
    & Follow standards; Try to learn more as the time passes;
    & No
    \\ \hline

    \textbf{7}
    &
    Complexity of the project
    & 2
    & 3
    & 6
    & Document Everything; Simplify everything;Join Communities;
    & Yes
    \\ \hline

    \textbf{8}
    &
    Unknown Errors
    & 2
    & 4
    & 8
    & Join Communities;Arrange more debug mechanisms;
    & Yes
    \\ \hline

    \textbf{9}
    &
    Acceptance
    & 4
    & 5
    & 5
    & Make the best solution possible;Make clear the need of the solution;
    & No
    \\ \hline

    \caption{Risk List: General} \label{tab:activity_schedule}

\end{longtable}

\subsection{Relevant tools}

Before delving into the use cases and what were done in practical terms, there is a need to explain seamlessly which tools were
used during the project. This is because, by explaining what was used, by extension, an assumption by the reader for the reason why
certain things were inserted in the project is achieved. Some tools are related to blockchain, since that is the main subject under
scrutiny along with the healthcare sector, but others, on the other hand, are extensions of a product that is strong by itself. Reasons
for the usage of blockchain were already mentioned in the State of Art; thereby, there is no reason to go much deeper in this aspect, but
returning to the other tools, they are needed for commodity purposes, and probable risks resulted by the usage of certain tools will be
covered further in this thesis. Also, it is important to mention that the idea around this project is not to achieve excellence in
decentralization or to simply use emergent technologies because they are trendy, but to try to solve the existing problems with the best set
of tools that are proven to be the best in the current market and perhaps standardize the way private blockchains are designed to gain broad
acceptance. Standardization helps with broad acceptance because this way implementations are not that fuzzy like they were before,
contributing to ease in implementations and creating a common path between people that, in case of obstacles, there may be a current
solution already because someone crossed the same path before. This is essential for a vast spread adoption, and thereby that's our
contribution here.
There are 3 covered sections within the tools; the list is not at all complete, but there is an effort to cover the most important concepts
of technologies that were used during the project. This will empower the reader with the correct view of what has been done so far.
There is a Microservices section in which the tools are a part of the other tools that will be covered. They retain an honored representation
on this project because their usage is meant to empower administrators with capabilities such as resource management, easier setup of
networks, decoupled services, scalability, and high availability.
Another important section is analytics; this is very important for any type of robust system, and its aim does not reside only in
maintainability. It empowers the project with also mechanisms to detect bottlenecks, detect unexpected behavior, troubleshooting support,
debug support, benchmarking, observability, measures to better resource management and more.

Finally, there is also the web development section, which is used in this project as the core to build extra services that may be required
for the normal function of the network. They work mostly for communication support, for ensuring security, and to help automate procedures
during the results obtained in this research, but also for future usage while using this in a real environment.


\subsubsection{Blockchain}
This section will contain all necessary related or close related information about blockchain that is being used in this research. The
aim in this section is to give a good fundamental overview about Hyperledger Fabric, the private blockchain technology that
is used all over the investigation and also to give some hints about IPFS, which despite not categorized as a blockchain, has a
lot of characteristics that resemble a blockchain thereby despite the section name, it still makes sense to put it here for the purely
sake of relation between the concepts making it easier to digest. The first is considered a general purpose technology,
which can be used with all kinds of data but having the capability of working with all kinds of data and at the same time be capable
of dealing with all types in a effective way that's a completely different story and, because of this and also to respect to files, there
was a need to think in a extension of the blockchain capabilities where IPFS can effectively play its rule. Altought intriguing,
in this work there was not a measurement of performance regarding this technology and how it would behave in the private landscape, which
means that at least in this realm there are no tangible evidence of effectiveness on our side, which may result in the future measurements
while working interchangeably with both the blockchain and this technology for storing files, exactly like describe in one of our future
mentioned use cases.

\paragraph{3.5.1.1- Hyperledger Fabric}\mbox{}\\
Fabric is an open source permissioned distributed ledger platform designed for use in enterprise contexts, delivering a
certain number of capabilities that are imperative in business scenarios. It is maintained by the Linux Foundation, and it presents
an architecture that is modular and also configurable. It supports smart contracts through general purpose programming languages
like Go, is permissioned, has pluggable consensus protocols that range from existing default ones to custom, does not require
a native cryptocurrency and offers one of the better performing platforms today of this kind for transaction processing and
transaction confirmation latency \cite{hyperledger}.

This framework will be intensively used in the provided use cases. With this in mind, this section will be a solid coverage all over
the main concepts of this framework.

Relatively to Permissioned Networks, this is a concept relative to a type of blockchain. Dissimilar to public blockchains
such as Bitcoin and Ethereum, often called permissionless blockchains, these ones
restrict access to a set of known participants, making them ideal for applications that require trust and control over the
membership. Like mentioned before, Hyperledger fabric is one of these types, and so is Corda. Both
platforms emphasize accountability without relying on a central authority, enabling organizations to construct custom
and heterogeneous applications where there is a clear conscience of who can participate in the network \cite{permissioned-blockchains}.
Usually, the usage of one restricts the usage of the other.However, there are also approaches where networks
may want both permissionless and permissioned, referred to as two-tier blockchain, where both
types are combined to allow both participation and validation, such as supply chain or cross-border financial
transactions \cite{two-tier-permission-blockchains}. For the context of healthcare, permissioned is obviously
more appealing, such that operational efficiency and transaction security when combined with restricted
trust between participants is needed for security and data privacy \cite{permissioned-vs-permissionless-tradeoffs}.

Furthermore, there is also the modularity of Hyperledger fabric to be covered. It has 5 components: the Peer,
Orderer, Chaincode, Ledger, CA's (Central Authorities), and client.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/tools/hyperledger/hlf-minimal-setup.png} % Change this to your image file
    \caption{Major components Relationship}
    \label{fig:sample-image}
\end{figure}

As an overview above, there is an image that illustrates not the communications but how components
may relate with each other. Also, it should be noted that this is very far from representing a whole system, where usually
multiple peers must endorse transactions and multiple orderers must order transactions and blocks. Putting in another terms,
the representation here explained is just for the sake of understanding how individual components work with each other. With
this in mind, each component's individual functions will be covered more ahead.

The Peer (orgx-peer1) in this representation is the middle point of everything. This is because it is the one that receives
the transaction proposals and endorsements that come from other peers by previous request of a client. It also receives blocks
from the orderer, and it is responsible for endorsement of transactions by using the chaincode, and furthermore,
after validation through this endorsements, it is the one that receives a block containing valid transactions
from the orderer (orgx-orderer), for then to include in it's ledger (orgx-couch-peer1). The client, on the other
hand, manifests in two ways: the client itself as an actor and the client-api, which turns out to be the application
itself that is responsible for calling the Peer. The chaincode (chaincode-as-service) simply runs logic, which is
what the Peer endorses, by running the application against the ledger (orgx-couch-peer1). In respect to the
ledger, it is used as a mock to endorse transactions to see if the output is the same within the vast majority
of peers.The orderer receives transactions and puts them inside of blocks, and after ordering these transactions and
blocks, it delivers the blocks to the peers. Finally, the CA(certificate central authority) only gives cryptographic
materials so that components can identify themselves within the network. It must be known as well that the chaincode-as-a-service
has the option to create TLS connections with the peer. In this picture there is only one CA for purposes of simplicity on this
explanation, because usually there can be a chain of CA's and in each level, for purposes of good practices, should have 2 CA's
where one is for normal certificates and the other is for TLS. The difference between normal and TLS certificates is that
one is for identification and the other is to secure the communications with encryption. Once again, in this representation only
normal certificates are considered, but within the network there are both, which contribute to a secure
environment. \cite{rfc5280}.Now that their relations were described, let's describe them in more deep terms for further explanation.

In what concerns the peer, this is a fundamental concept within the network since it is responsible
for managing ledgers, smart contracts, transaction proposals, and endorsements. Ledgers and
Smart contracts will be explained further, but a transaction proposal is a transaction that was signed by
some entity that wants to be further endorsed by the peers for then to be included in the ledger if valid. Endorsements, on
the other hand, are the process of a peer running the logic within the proposal against its ledger. Depending on the configuration of
the channel and the number of peers that validated this proposal, this transaction will be included on the block
or not (within the orderer side); therefore, it could be or not included in the ledger depending on its validity.

In the Orderer sphere, however, there is the responsibility of receiving endorsed transactions, which,
depending on the number of signatures of the peers in it, could add the transaction to a block or reject it. In the case of the first,
after ordering the transactions and the given blocks, it returns to all the peers this given block. Block this that will be used
for the peer to insert its operations against it's ledger, therefore ensuring that every single peer has the same view of the state
because they are all running the same operations against the same state. To achieve this, the orderer uses what is called a
consensus protocol. This protocol is a mechanism to reach agreement in the sequence of transactions, but that will
be discussed further.

Another vital pillar for the network is the Smart contracts, more commonly called Chaincode. This is where all of the
business logic runs, and it uses abstractions to communicate with the ledger. Usually, installation is required only once. However, it
must be added per channel if that is the intention of the participants within that channel. This means that I can have the chaincode
installed, but this chaincode could potentially not be allowed to be called because, for that, it must be proposed to be added and
committed when a certain number of participants agree on its insertion. Besides that, there are 2 typologies for usage of
chaincode: normal chaincode installation, which occurs per peer and the chaincode becomes more like a side
container, and chaincode as a service, where the chaincode becomes a service and to install it there must be passed metadata
for the connection management. The normal way is often used for questions of ease, and, along with it, there is the certainty that
the chaincode that is being invoked is in fact the desired one, since it is literally a side container or side service of the peer,
therefore also giving more isolation to it. The other way, however, depending on how it is deployed, it can be reused by
multiple peers, which means that if an operator does not retain this service within its infrastructure, it may represent a
risk since the endpoint could be the same and its application could be changed. Be mindful that, in case
the chaincode as a service is adopted, depending on how it is implemented, it can be exposed to the common microservices
vulnerabilities, and further actions should be taken to prevent these \cite{microservices-common-vulnerabilities}.

Ledger, on the other hand, is a normal database like couchdb \cite{couchdb}. This is the state that the
Peer uses as a mock to run its business logic and get an output for later usage for the endorsement procedure. Transactions
can be rejected by either violating some business logic or by presenting different outputs among the peers. As an example, let's suppose
there are 3 peers. In the first situation, peer 1 rejects the transaction because the business logic says so. The other 2 peers, peer2 and
peer3, respectively, accept the transaction. In the case of a majority policy, the state resulted in peer2 and peer3 being included
in peer1 as well, simply because the majority wins. Also, let's consider the case where peer 1, peer 2 and peer 3 have different outputs,
but the signatures were still reunited because the transaction was not rejected. What will happen is that when this transaction reaches the
orderers, the orderers will reject its validity. This is because, despite passing the peer endorsement, the orderer endorsement will fail due
to a lack of determinism between the three peers. Also, within the same database, there could be different slices of data for
each channel. However, the concept of channel is something that should be delved deeper into more ahead.

Speaking about CA's(Certificate Central Authorities). This is a service that serves cryptographic material like private
keys and x509 certificates, which within a hyperledger fabric network form a MSP (Membership Service Provider). MSP
is a mechanism designed for providing a way to offer identities that could be trusted and recognized by the rest of the
network. To achieve this, there is the usage of this cryptographic material provided by the CA's where a key
pair is offered. One of these pairs, the private key, is used to sign transactions, and the other, which is public, gets shared among
the entities that will be communicating with the owner of the private, giving them a way to verify that this signature is actually
valid. Hyperledger lets the operator either rely on existing public CA's, use their CA implementation, or simply use
a hybrid methodology. The main difference between the public and their implementation is that it's implementation already
comes with a given file structure to make the configurations easier and also the aggregation of more responsibility to the operator, but
that's something that is not the concern here, thereby it should not be extended.

Relatively to the client within the hyperledger fabric components sphere, there is a close relation in
how it operates with the chaincode. Like chaincode, both can be implemented with a certain number of
high-level programming languages such as Go. Both use an interface, attributed to their set of tasks in aim.
In the case of the chaincode, its interfaces are related to making interactions with the ledger, but in the case
of the client, its abstractions are focused on making calls against the peer by giving metadata related to
the channel and methods to be targeted. Thus, it’s more formal name is Fabric SDK, where there is the
opportunity to use Node.js, Java, and Go. Giving the opportunity to perform various functions, such as
creating channels, submitting transactions, querying the ledger, and managing the chaincode. Within
the SDK ,there are various subpackages aimed at serving different purposes, like: gateway, for establishing
connections to the network and provide access to channels and contracts. Network, which represents
a channel that a client can interact with. Contract, conceded for iteration with a given smart contract
deployed in a channel, used for submitting and evaluating transactions, and finally Wallet, responsible for
managing user identities and credentials for connecting to the fabric network. Further details should be
seeked within the proper documentation of hyperledger fabric.

Apart from what has been covered so far, there is the concept of channel. This concept is very important since it is the existing
mechanism that allows you to aggregate a certain number of components, thereby creating a linkage between them and a precise section for
them to exchange information. Within the documentation of Hyperledger Fabric, there is the reference to channel
as a group of friends. This is because, exactly like a channel, you have multiple groups of friends on your social media and
their members are different depending on the task or the theme to be discussed within that group. Also, within those groups there is an
exchange of information that is private and it's not shared with other group members, which is essentially the same thing as a channel. In
a channel, you aggregate a certain number of components that could be part of multiple different organizations depending on what you want
to be sharing within that channel, and also within this channel, there is the option to create a private collection. Despite the
theme being the same, there are participants that may want to discuss information regarding this discussion within the group but
just with a set of participants, especially like it would be with the private collection. Though the difference with a friend group is
the nature of blockchain. This means that with a private collection, there is the option of creating a subset within the channel, making
others able to see that you are communicating with each other but not what they are exchanging. In other terms, a given organization can
be in multiple channels, with multiple different individuals in each, and it can also create multiple private conversations regarding
the same theme, but just with a subset of participants in the same channel and exclusively of that precise channel. As an example,
let's think of an organization that has the idea of creating a group of medicament's suppliers and other group of medical clinical
reports. Obviously, in these examples, there will not be common participants because their nature is different. Imagine the organization
discussing within that group that it needs medicament's for X. Supposing they want to buy the cheapest one, they will request it from the
enterprise Y. This enterprise, although offering it more cheaply, may not have the quantity to satisfy this need, making this organization
forced to go to supplier Z to satisfy the rest of the demand. Maybe, for the benefit of this organization, prices of the product
should not be known by the participants within this channel, which means that at least the monetary transactions within this channel
should be done in private collections. However, the transactions of products could be done publicly, therefore making it easier for all
of the participants to have the information publicly available that this organization is supplying itself from these two organizations,
making others in the obligation of trying to bargain to also benefit from this channel. Also, the same applies to the clinical registry,
even if it has a different logic or idea. There may be information that can be publicly exchanged between doctors, but there may also
be communication that should only be sent within a more restricted set of individuals within that consortium. With channels and private
collections, hyperledger obtains data confidentiality,selective data sharing and regulatory compliance \cite{private-collections}.

Endorsement Policies are another big dinosaur in the room when speaking about hyperledger fabric. These are the
rules under which the transactions will need to apply, making the transactions valid in case they succeed and invalid in case they
fail to do so. In other terms, these are the policies that define which peers must sign (endorse) a transaction before it can be valid
and aggregated to a block by an orderer. It specifies how many and which organizations must approve the transaction before it can be
inserted into the ledger.

Piercing to the real use cases realm, there are several occurrences of hyperledger appliances. The sectors affected by this remarkable
technology were: Financial,Healthcare,Supply chain management,Government and Public Sector and
Insurance. Within the Financial, various banking consortiums were created for processing cross-border payments, managing
the financial chain, and enabling transparent asset transfers \cite{finances-use-case}. In the Healthcare there are also a
lot of occurrences of it's usage for managing electronic health records (EHR's), which is possible due to the channels and private
collection capabilities, allowing sharing of patient data between healthcare providers \cite{healthcare-use-case}. In the
Supply chain management, there are multiple occurrences for the usage of hyperledger fabric for tracking goods. By using
this, enterprises were dotted of track food products from the farm until the shelf, reducing the time required to trace the origin of
food products in the event of contamination while still offering privacy for sensitive business data using private collections
\cite{supply-chain-use-case}. Also, when it comes to Government and Public Sector, it is well known that there are already
present in the market implementations for the usage of such technology for managing digital identities, land registries, and transparent
voting systems \cite{public-sector-use-case}. Finally, there is the usage for Insurance, where there are retained implementations
that are meant to help claiming processing and reduce fraud. Thus, by levering this, the Insurance can collaborate in a
shared ledger to verify these claims and manage shared data, reducing duplicated and fraudulent claims \cite{insurance-use-case}.

\paragraph{3.5.1.2- Ipfs}\mbox{}\\
When it comes to IPFS, it must be known that this does not represent a blockchain, though its concepts derive from a
P2P perspective, which means there are relations between the two. It is a modular suite of protocols for organizing and transferring
data \cite{ipfs}.

More concretely,IPFS is a tool that stands for Interplanetary File System, where it refers to an implementation that was meant
to become the file system of the future.

This solution can be interplanetary, because when the first request of a file in the network occurs, it may endure a lot of time, but
after getting it, the file will be retrieved faster because it will be stored in cache in the most nearby peer. Thereby, if you plan to
travel to another planet, it can be possible to share files in this network, which, depending on the range, may also include other planet
files, but this has countless ways to occur. Or either by one of the peers from another planet changing its location to a location that is
reached by our network or simply because the connection between those 2 planets is feasible. However, in order for this to work, there is
still work related to making sure that the files are pinned; otherwise, they will get deleted later by garbage collection. This ensures
that we do not have files permanently. If they are not that relevant, with time, they will simply disappear \cite{ipfs-overview}.

\subsubsection{Microservices}
Within this section, there is information about the major high level technologies that were used in this precise research that are related
to microservices. One of those technologies is almost a obligation for the current landscape of technology, there is one which
is a upgrade, one that is necessary in certain concrete situations and there is also a extension but all of them are valid. The one that
is almost a obligation is docker, because currently in the tech landscape it is used in almost every system because of its isolation
therefore faster for development in big teams. The upgrade is the kubernetes, which manages this docker images like they are
resources ,making it complex in a certain way but also easier to manage in another after learning it. The necessary in certain concrete
situations, in another hand is MetalLB, because it is meant to on premise implementations, which means that it is more suited
for own infrastructure implementations which is a big help when it comes to healthcare due to their resistance into moving to
the cloud. Finally the extension is Istio, because it is used to have more control over the network and its communications, but
its usage its not mandatory, thereby in this project despite tested it is not used in any implementation.
After Hyperledger, this are the technologies that more represent how some implementations communicate, therefore is good to
present what they are and how they work, so the reader can understand the architectures that will be presented further.

\paragraph{3.5.2.1- Kubernetes}\mbox{}

Kubernetes stands as a open-source platform designed for automating the deployment,scaling,
management of containerized applications,management of resources, management of secrets and
management of configurations. Originally developed by Google,kubernetes has become a standard for
managing containers being a premise to build cloud-native applications. By taking leverage of this, developers can abstract the
infrastructure and focus more into the application logic instead of hardware and other operations side tasks. With this in mind,
kubernetes today is centered as the pilar for building any application, even if it is on-premise(within a organization
own infrastructure), due to the fact that currently every organization seeks a microservices architecture which obligates
businesses to build their own kubernetes clusters. This is because, microservices itself offers a lot of advantages for their business
such as high-availability because of deployments, better resource management and resilience which are crucial
for any organization that wishes to remain competitive \cite{kubernetes}.

\paragraph{3.5.2.2- Docker}\mbox{}\\
Docker, on the other hand, is a technology that could be abstracted by the previously mentioned Kubernetes and is
said to be a tool to build applications in a different paradigm than before. To contextualize, in the past it was something normal to
install dependencies directly in your local machine, which created some burden since you may have had different projects with different
dependencies versions. Also, the application would mess with the local machine configurations because it was directly using it's resources,
having more probability of making adjustments that would compromise other projects by having different configurations. By acknowledging these
limitations, Docker came to light because it was conceded to automate deployment, scaling, and management of these referenced applications
using a new concept called containerization \cite{docker}.

\paragraph{3.5.2.3-MetalLB}\mbox{}\\
On-premise cluster is an advanced concept related to setting a network of computers within your own infrastructure. In the case
of this project, the reference to this is related to a network composed of kubernetes nodes. Because of this, it is imperative
to mention MetalLB, which is a load balancer conceded just for on premise Kubernetes clusters \cite{lb}.

When it comes to kubernetes, usually the tendency is to relate it to the cloud. However, as the time progresses, components
become more powerful and cheaper, and more and more, the consideration of moving to the cloud or not becomes more a question of cost and
responsibility. This is because, though the power of components is becoming stronger, the complexity of hardware is sometimes increased,
making organizations choose between being themselves responsible for the maintainability of the infrastructure or simply giving it away to
those that are more specialized in the subject, which may or may not reduce costs for them because not only do they have the common
infrastructure costs but also the burden of finding and maintaining qualified personnel for this type of job \cite{onpremice-cloud}.

MetalLB comes into play when it comes to those more audacious to have all the responsibility. As a strong Load Balancer
for on premise networks, it stands in this project as a marvelous tool for achieving load balancing, which is a concept
where the requests are distributed through multiple instances of the same application, balancing this way the load that each may have. Cloud
distributors have their built-in support for such, while on-premise networks do not, which makes MetalLB a must for setting
such capability in a non-cloud environment.

\paragraph{3.5.2.4- Istio}\mbox{}\\
In this thesis,Istio appears more like an extension and not concretely as something that was of extreme usage. However,
because it was used to determine in each measure is useful and also because it is very different from other concepts, explanations
over what it is are required because, although not extensively used, it is mentioned in the thesis \cite{istio}.

Microservices is an architecture of exalted power. However, what comes with great power also requires great responsibility. Therefore,
conducting the management of the communications between services is not a straightforward task. By virtue of such, the more
the application expands, the more work there is around this matter, making DevOps teams struggle when it comes to make
optimization and manage this within the application realm.

By means of this, Istio surges as alleviation of such. At the hand of its capabilities, it serves as a service mesh, which
is nothing more than an abstraction created around the application for providing Traffic Management, security,
Load Balancing, and monitoring.

As mentioned before, cloud-native architectures and microservices are becoming something permanent. Such that managing it is a challenge. Thus,
using a Service mesh could potential be something meaningful because it works as an abstraction layer that handles the
dynamic networking and communication between microservices. With this brief introduction in mind, thinking in the most important aspects
for which this is needed is something intriguing. Firstly, enhances microservice communication and networking is one of the
aspects for including istio. This is because, a service mesh provides service-centric networking, thereby oferring features that
are not native to kubernetes. This is helpful for managing services communications with capabilities such as load balancing,retries,
timeouts and circuit breaking \cite{service-mesh-enhance-communications}. In second, despite current implementations create
overhead in terms of resources, due to the proxys and the new side containers, there are service mesh implementations that actually aim
to improve performance and resource efficiency. This can be done with new architectures like FlatProxy, which reduce
performance bottlenecks and improve resource utilization \cite{service-mesh-flat-proxy}. As a third motive, there is the aspect
of Observability and Monitoring. This has to do with the fact that a service mesh collects metrics,traces and logs for each
service interaction, providing insights about the behavior of the microservices enabling performance bottlenecks spotting, understanding
of service dependencies and improving system debugging \cite{service-mesh-monitoring}. In forth, it should be known that it also facilitates
traffic management through fine-grained control policies, such as traffic splitting,canary releases and fail-over strategies. However,
there are studies affirming that it creates overheads such as 269\% latency and 163\% CPU usage, emphasizing the need for
optimization \cite{service-mesh-traffic}. Finally, Security and Resilience is another aspect that serves
as a benefit to use it since a service mesh usually offers mutual tls out of the box with certificate rotation \cite{service-mesh-security}.

\subsubsection{Analytics}
This chapter centers it's focus in Analysis. When there are ideas about a given system, usually people don't think right ahead on this
matters. Despite the possibility of creating systems without such capabilities, it will become clear further in time that it will be
painful and more like a burden if there are none, this is because, the more complex the system gets the more there is a need to auditing
and this is only possible if there are mechanisms prepared for the analysis of the network. In other words, despite not directly mandatory,
it is almost a obligation to make our infrastructure doted of such, because by having good design over the analysis, auditing becomes
more easier and there is the possibility of knowing where to do better by spotting, for example, bottlenecks but also where in the system
something is failing (debug). Also, there is a need of having tools that simulate real behavior from users so that the responsible knows
the limits of the given System. With this in mind, in this section, it will be covered all of the tools that in conjunction are used to
deliver this capabilities that were just spoken.

\paragraph{3.5.3.1- Caliper}\mbox{}\\
In what respects the domain of Benchmarking. It is well familiar that it represents the
measuring and comparing of performance. As the need of metrics arises during a project, there will be space
for such concept and within this thesis this could not be different \cite{caliper}.

In prospect of the usage of such concept, within this project,there will be justification of results. Thereby, the need to use a tool
such as \textbf{Caliper} and the requirement for further clarification of it, which will be stated in this category.


Caliper \cite{HyperledgerCaliper} is an open-source multi-function benchmarking tool. It is 
part of the hyperledger project and allows it's clients to evaluate various aspects of their blockchain 
implementations. This aspects could be transaction throughput, transaction latency, resource utilization 
and even scalability. Thought it's modular architecture, it is possible to implement a vast number of custom 
connectors, enable the developer to connect in various ways to multiple blockchains. Also, it has a set of 
standard connectors for a lot of network tastes such as Hyperledger Fabric, Ethereum, Hyperledger Besu 
\cite{HyperledgerBesu} and FISCO BCOS \cite{FISCOBCOS}. This flexbility allows those that have power 
over it to compare different solutions. Key features of it are: Benchmarking Across Multiple Blockchain Platforms, 
Customizable Performance Tests, Detailed Performance Metrics, Modular and Extensible Architecture, Automated 
Benchmarking and Visualization and Reporting.

\paragraph{3.5.3.2- Prometheus}\mbox{}\\
Prometheus is an open-source monitoring and alerting tool designed for recording real-time 
metrics. At the essence of this thesis, there will be instances where Prometheus is mentioned. This is 
because it is very useful to gather data from various places at the same time, making it easier to grab the 
data at a single point of failure. In other terms, it is a tool based on time-series and it remounts to the 
collection of metrics from various systems and services, making it very suited for this kind of project. Also, 
it provides various useful features such as Time-Series Data Model, Pull-based model, PromQL, Alerting, Service 
Discovery, Multi-Dimensional Data, Efficient Storage and Dashboards and Visualization \cite{prometheus}.


\paragraph{3.5.3.3- Cadvisor}\mbox{}\\

As mentioned before, containerization is a concept for package applications. It creates lightweight,standalone  and executable applications,
while giving to them all the necessary dependencies to work in a portable,scalable and isolated way. By the effect of such, containers
have gained an extreme importance in the current sphere of software engineering,making them of a big magnitude of importance to every single
application that needs to be done nowadays \cite{cadvisor}.

With this in mind, and within the curious landscape 
of congregating measures, there is Cadvisor as a very useful tool that was 
built by Google. It has enormous advantages when it comes to the collection of data 
from insights about containers. By collecting and exposing metrics such as CPU, memory, 
network and disk usage, it helps operators to monitor resource consumption of containerized workloads. Also, 
it works like a glove when combined with Docker, making it a valuable resource for providing a complete 
observability of containers. It's features range from Container Resource Monitoring, Per-Container Isolation, 
Historical Performance Analysis, Container Health Metrics, Integration with Monitoring Tools, Low Overhead 
to Kubernetes Integration, making it imperative to be delved into deep terms in this section.


\paragraph{3.5.3.4- Grafana}\mbox{}\\
For showing data within the Benchmarking landscape, there is the honored mention of Grafana. This is an 
open-source platform for monitoring, visualizing and analyzing real-time data. Thus, alongside Prometheus, 
it's useful to gather data that was collected during tests of performance, but this will be covered better 
in a future discussion. With this in mind, it is valuable to debate such tool into this segment \cite{grafana}.

When abording the data sources and integrations of Grafana, it should be known 
that it supports multiple data sources, making it easier to create visualizations, 
dashboards, and even alerts. The supported data sources are prometheus, which was covered 
already in the previous section; influxDB, which is another time-series database used for 
monitoring and alerting; Elasticsearch, which is a distributed search and analysis engine usually 
used for logs and unstructured data, making it worth when searching and visualizing logs and event data; 
Graphite, a monitoring tool used for storing and visualizing time-series data for analyzing system performance 
metrics; AWS Cloud Watch, for monitoring resources and applications that belong to AWS; Google cloud monitoring, 
providing monitoring, logging, and diagnostics for applications hosted on Google Cloud; Azure monitor for 
monitoring Azure cloud resources; SQL Database, allowing querying and visualization of relational data like 
those from MySQL, Postgresql, and MS SQL; Loki for aggregating logs, enabling log collection, search, and 
visualization in Grafana alongside metrics; Finally OpenTSDB for scalable and distributed time-series data. With 
all these integrations, Grafana stands firm as a universal platform for showing data, making it feasible for this
project.
\subsubsection{Web Development}
To build any kind of application, there is a need for standard suitable tools. Since the projects involve 
creating multiple microservices,there is a huge dispersion in terms of frameworks. This is because, some 
technologies are better for certain tasks and others are more suited for other kind of use cases. There are 
cases where within the project it can be spotted 2 to 3 different programming languages and with
that also different frameworks. This is because some are better for certain API's (ex: Kubernetes API) 
and others are better for faster build and development in containerized environments (ex: quarkus) which is fine 
in one hand, if considered that microservices usually means heterogeneous services but it's a struggle in terms of 
complexity, because it requires a high skilled set of individuals
within the team, all to handle that context switch.

\paragraph{3.5.4.1- React}\mbox{}\\
If further examination around the questions under research is performed, particularly the second, then it may be expected
that there is a UI evolved. Not as a obligation, but more like something that could eventually occur. In the case of this project,
there will be mentions around creating one for management purposes of a infrastructure, but more will be revealed as the use case cases
come closer. Thus, explanation of the given tool that achieves this must be taken \cite{react}.

\textbf{React} is a tool to build web applications, centered in the paradigm of reusing multiple components, components this,
that are modular and can be used in multiple places of the application. Instead of using the traditional way of creating a HTML
code for each page, with this technology there is the option of for example using a piece of code that represents a top bar and
apply this same top bar in multiple pages, having this way to write less code and also make the application easier to develop. Also,
it is easier to make components render in the desired way when data changes occur.

\paragraph{3.5.4.2- Keycloak}\mbox{}\\
When it comes to management of access to services, Keycloak surges as a valuable asset. It is an open-source
identity and access management (IAM) solution aimed at modern applications, services, and APIs. Offers
authentication, authorization and identity federation services. In this section, there is a deep dive
into microservices, and having a technology that could create means for authentication when dealing with a determined number of services
that get exposed at a global level is important. With this in mind, let's dive into the main capabilities of such a valuable
arsenal \cite{keycloak}.

Like previously spoken, Keycloak is an identity and access management tool (IAM). Which means that it has processes
to manage digital identities and control access to resources within an organization. To accomplish this, security policies are enforced,
authentication and authorization are secured and compliance is also expected because there is a management of who has access to
what resources. The key functions of IAM are authentication, authorization, user management,
single sign-on (SSO), which stands for allowing users to log in only once and keep the access and federation to
authenticate users across multiple identify providers.

\paragraph{3.5.4.3- Quarkus}\mbox{}\\
Quarkus is an open-source Java framework, designed specifically to optimize Java applications for Kubernetes,
containerized environments and cloud native development. It's usage is still gaining presence due to its premature
environment, but it does show to be valuable when it comes to building and starting up containers, and that is the reason why it is
called the cloud native Java solution. By working seamlessly with GraalVM, which is used to compile Java apps to native executables,
it reduces the start-up time and memory consumption. Also, there is also included the live reloading feature,
which, despite being useful because there is sometimes the need to change the application within the cluster, may be a bit worthless because
of the existence of technologies such as devspace since it allows programming within the cluster and at the
same time runs tests like it would be done in a local machine. But with this in mind, it must be said that this tool does improve
productivity and it was a powerful tool for the arsenal of this project \cite{quarkus} \cite{quarkus2}.

\paragraph{3.5.4.4- Graphql}\mbox{}\\
Within the landscape of web development, there is always the urgency of adopting an approach for the implementation of APIs. This is of
extreme importance and can be difficult due to all of the aspects that the given tasks may take. Ranging from a vast number of
characteristics, the ones that should be more taken into account when choosing which methodology to use are Data Model
(if it should be RPC-based, resource-based or query-based), data format, protocol of transportation, performance,
if it should have real-time data supply and typing. With this in mind, within this section it will be discussed
GraphQL.

Graphql is a mid-level, high-performance approach for designing API's. It is query-based,
supports JSON, has a single endpoint for making the queries, runs in http,
supports real-time with subscriptions, and it is strongly typed. It was invented by Google, and it is used usually
for complex queries and dynamic data requirements use cases like Social media apps, dashboards and
even mobile apps. The reason for exceeding in this area is because while in REST, there is the request of all parameters
of a model, in this approach you only request what you want from a model, therefore with this approach there is better
performance overall in the client side and more overhead in terms of CPU for only returning what was requested by query. As an example,
let's think of a model Person, where the parameters are: name and age. With this approach and by using queries,
there is the possibility to only request the name and leave the age behind. At first glance, it appears that this has not much relevance,
but when it comes to thousands of records and full of relationships, this approach becomes meaningful, and if mobile development is
considered, it becomes even more so because of the even more limited resources when compared to a computer. Thereby, the usage of such is
indeed useful because, although the resources within mobile are becoming hugely better, there are still a lot of people that cannot
afford to have good resources, which could potentially exclude them from using their favorite applications,
be it either Facebook(the one that created this approach) or X \cite{graphql}\cite{graphql2}\cite{graphql3}.

\paragraph{3.5.4.5- Rest}\mbox{}\\
When it comes to the most widely used approach for designing API's, REST is the way that is more present in every single
organization. Though less performant in a lot of occasions, it is still the most standardized one, thereby the most used and more
accepted. It is resource-based, has multiple endpoints as a source of data, uses HTTP
and is typically loosely typed. The best use cases for this type of way are Simple CRUD operations and
resource-centric API's which, more concretely, correspond to web services and public API's
applications \cite{rest}.


\paragraph{3.5.4.6- gRPC}\mbox{}\\
Delving more into Microservices tools, the gRPC approach can be seen as a valuable arsenal for that sort of applications.
This is because this way of creating APIs is the most efficient because it's presented at a at a lower level, which can be a huge helper
for extending networking communications between services. Coming from Google, this is an RPC implementation. The g stands
for google and RPC for the remote procedure call. Usually seen as the modern implementation of java RMI,
it works with protocol buffers which is more performant than json, it has a single endpoint with method-based calls,
uses http2, it's high-performant and lightweight, supports real-time share of data with bidirectional
streaming and is strongly typed. Also, the best use cases for this sort of approach are high-performance,
low-latency communication, and microservices, with common applications being microservices,
real-time streaming, internal API's, and extension of traditional application communications with more
modern ones. This approach exceeds this project since it is very good for extending communication capabilities and is also
the protocol used to build hyper ledger fabric \cite{grpc}\cite{grpc2}.

