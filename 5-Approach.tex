Since the methodology used has a bearing on the success of the project in hands. It is of high importance to choose which practises should be adopted to effectively move forward. By means of this, investigation under which arsenal were to use is counseled. After a lot of digging, it was decided that the number of different practises should not be to vast, otherwise it would be a burden to put them to work together seamlessly. 

Having a research methodology is full of advantages like: helping \textbf{plan},\textbf{structure} and \textbf{conduct} the research in a way which is structured and objective. 

Research methodologies can largely be categorized into \textbf{quantitative},\textbf{qualitative} and \textbf{mixed}. \textbf{Quantitative} focus on gathering numerical data and performing statistical analysis to draw conclusions. \textbf{Qualitative} on the other hand, is more suited to specify detailed insights such as behavior,motivations and experiences, which means that is more theoretical while the \textbf{quantitative} is more empirical. Finally, there is also the concept of \textbf{both} which stands for having both \textbf{quantitative} and \textbf{qualitative}.

During this thesis, in terms of research methodologies, there will only be space for two practises:  \textbf{DSR(Design Science Research)}, which in terms of nature of findings is hybrid, thereby categorized as a \textbf{both} approach like mentioned in the last paragraph and \textbf{SWOT} which is more \textbf{qualitative}. The first one is more complete and more suited for the projects while the second will be used more to drive the conclusions of the project to further analysis of it's pros and cons of usage of the project in cause. With all of these settled, lets move to deeper thoughts about each.

%
%
%    DSR
%
%
\subsection{Design Science Research (DSR)}
As an important gadget for the project, \textbf{DSR}  was the embraced tool. Considering it's traits, resembling extreme focus on solving real problems, it represents indeed a powerful tool for going on with this thesis. Some people gain knowledge through reading while other's achieve so by earring,practising or even speaking with other's. It's a serious challenge understand how a team will work best but,due to the nature of the team, \textbf{DSR} proved to be the best choice for it's phenomenal practise and rebuild model. In a distinct set of terms, this methodology is notably better for individuals which praise more the practical way of things and by it's landing into the creation of artifacts for solving the current landscape of problems, creating this way solutions and still gaining theoretic knowledge, it does outpace other methodologies making it better for the expected aspiration of this project.

\subsubsection{Goals of DSR}
For the verbal reference of the \textbf{DSR} goals, there is a colossal number of concepts that could be spotted as one. The gross of it, outcomes from the requirement of appeasing certain field such as information systems,engineering and others. This project is within the information systems realm, which means that these values are more than fit for the project and thereby qualified to be mentioned here. By means of this, the most captious goals will be pronounced here.

At the heart, \textbf{solving practical problems} is one of those goals. This is reached by the design and creation of such research artifacts that are implemented into practise, where those are seen as the form of models,frameworks,methods,processes,systems and even tools. With accordance to this means, practical problems are solved and theoretical knowledge comes as extension.

Besides that, there is also the goal of \textbf{creating innovative artifacts}. It's presence is requested when it comes to push the horizon of existing knowledge and capabilities. As Innovation takes precedence in any kind of project, it could not be different when it comes to the realization of this research, standing as a must for the \textbf{DSR}.

Tertiary to this, surges the \textbf{contributing to Scientific Knowledge}. Which is conventional, if further thinking is done through the definition and name of the methodology. Knowledge occurs by extension when depared with the solving resolve. Thus, the more knowledge extended bigger the contribution and new perspectives and ideas may flow, creating a avalanche of future projects, instigating development and furthermore improving sectors.

Onward to the next objective, there can be \textbf{Evaluation and Validation}, since the evaluation itself is a pillar for the loop created within the \textbf{DSR} realm and so it is validation but, altought valid, it does not mean that it should be final. On this assumptions, rigid considerations must be accommodated, however, solutions keep maturing which explains the continuous satisfaction of this precise objective therefore nothing remains fully complete, which is expected from the point view of innovation.

Another objective important to consider is \textbf{Balancing Rigor and Relevance}, where it stands glamorously within \textbf{DSR} to make sure that nothing becomes to much theoretical until the point there is no practical implementation. This is a risk in the research world, where by applying to much importance to the theory category, the project starts loosing color into it's true serving purpose. With this in aim, exists there the importance of instituting a balance between the two dimensions, emphasing both the theoretical inception but also the practical. This becomes the best approach, since the practical determines the verocity of the empirical sphere.

Lastly, there is the existence of another aspiration, denoted by \textbf{Generalizability of Solutions}. It's priority resides within the sphere of making sure that, knowledge produced within the project can be adopted into broader contexts, from a deck of various industries or scenarios. This bearers a huge relevance, because by having countless scenarios of usage,new use cases are produced, thereby relevance of the knowledge becomes enormous and so it's contribution for the research world, which affects the world by extension, main desire of the research itself.

\subsubsection{Artifacts in DSR}
\textbf{Artifacts} are a paramount in the \textbf{DSR} methodology. It is labeled as any \textbf{designed object}, that is conceded for deal with practical problems that a research may be centered in. Relatively to it's existence, it takes care of not alone living as functional tool for combat problems, but also as a source of new knowledge. The achievement and evaluation of \textbf{artifacts} is what most diverge this methodology with others that only concern understanding and explanation.
In this section, it is planned to delve into each of the \textbf{Artifacts}. They all have their own particularities, but they are described as the following:

\paragraph{A. Constructs}\mbox{}

\textbf{Constructs} are the genesis block, for describing elements and solutions in the sphere of the problem. They provide a way of communicating the building models and solutions.

As an example, within the realm of information systems, constructs could include the terms "\textbf{user}","\textbf{data integrity}" and "\textbf{security protocols}".

\paragraph{B. Models}\mbox{}

\textbf{Models} on the other hand, act as an abstraction representation or even framework, that illustrate how things play within a given realm. They exist, so there is awareness, predictiveness and even cleanness of phenomenons, thus helping into guide developing new solutions.

As an example, there is the inception of a \textbf{model that can optimize the supply chain operations, which may include insights into inventory levels,lead times and even order quantities}.

\paragraph{C. Methods}\mbox{}

\textbf{Methods} are procedures or tactics, that have the power to dictate how to come up with a solution for a given problem. They serve well, when it comes to standardized common and multi used tasks. By following this guideline, tasks are made with quality assured, making the execution more effective.

As an example, within the software realm, a \textbf{new algorithm} can be given with distinction for ordering data in a more desirable way, by taking into accordance the needs of the scenario. This algorithm may not be the most efficient but the most efficient for this precise case, making it tailor-handed specifically to the scenario in question.

\paragraph{D. Instantiations}\mbox{}

\textbf{Instantiaton} is a occurrence, which results from the application of a given artifact. By having a instance of what was conceded during the project, there is a certain data gathering for the understanding of the artifact usefulness. In another words, this is where the evaluation takes notice and thereby there is the gathering of knowledge from it that can occur, either in a successfully instance or failure. In each case, since this is working under the \textbf{DSR}, further thinking must be taken to understand until each measure it can have incrementations or fixes. Putting in another words, this is the artifact that is responsible for feedback over the solution, making it a valuable resource within the lifecycle of the methodology.

A software application that implements a new inventory management system in a retail environment is an instantiation of the designed artifact.


\paragraph{E. Frameworks}\mbox{}

For the purpose of contributing with a set of high-level blueprints, all designed for understanding interaction between systems, resides the concept of \textbf{Framework}. This represents a well formed approach for combining concepts,models and methods.

As an example, this can be a \textbf{framework for decision-making in healthcare}. It might give insights about patient data,clinical guidelines and doctor input but if this becomes actually effective for medical decisions, that's what a real world incrementation of this product may reveal with the given usage. Also, like mentioned before, it can be incremented something more into it, since a product cannot always be considered as final.
// ROLES

\subsubsection{The DSR Process}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/tools/dsr/DSR.drawio.png} % Change this to your image file
    \caption{DSR Life-cycle}
    \label{fig:sample-image} 
\end{figure}
Entering into the process of the \textbf{DSR} realm, there must be a understanding that this is a structured and iterative methodology, focused on construction,evaluation and improvement of \textbf{artifacts}. The process is extensive, but it covers all the steps necessary from problem identification to evaluation. Since it works around incrementation, this process is within a loop processing always new form of solutions and knowledge, since it produces both.
Without further delay, this section will be going through each step, covering,giving examples and also refer to its objectives. 

\textbf{Problem Identification and Motivation} refers to the first step. Here, there is a preoccupation into picturing a real-world problem which seeks solution. To glean such information, various perspectives must be attended such as the current state,context and impact on the project under scrutiny. The spotted problem should be doted of meaning and with a given degree of relevance. The objective here is to gain power to articulate the problem clearly and make sure the team has the motivation for solve it. A good example to achieve this is in \textbf{healthcare, where there could be observed a problem with inefficiencies in patient management}.

As a second step, there is \textbf{Define Objectives of the Solution}. After problem identification, it is of huge gravity to think in a given set of objectives for the future solution. The objectives could be qualitative or quantitative, even thought the quantitative are better, establishing them in the beginning is hard,sometimes it is not possible and there are also situations where requirements already give them away. The objective in this step is to set expected outcomes and conduct obligations for the artifact that will be forged. A example for this would be setting as an objective, \textbf{creating a system that should be capable of reach 50 request per second. But it could a more simpler thing like giving good data for decision making in a healthcare facility}.

\textbf{Design and Development of the Artifact} surges as the third step. When landed in this step, there is work to develop the \textbf{artifact} (model,method,system or framework) that will be managed to meet the objectives created on the second step. With the problem and objectives on hand, the team must create,conceptualize and refine it's artifact to better meet the expectations. In another words, the objective is create a artifact that addresses the problems. \textbf{Developing a prototype of a healthcare data management system} can be a good example of what is expected in this phase.

 Moving to the forth step \textbf{Demonstration}, here the \textbf{artifact} is assigned to work, by either creating a simulation or simply putting it in a real world system to check if it does meet the expectations. This stage is critical, because it serves as proof of concept to inquiry if it has utility or not. Regardless of the result, knowledge is generated and the \textbf{artifact} has room for growth because of the feedback created in this phase. The objective is to demonstrate if the problem can be solved or not with this implementation by a practical setting. One example of this would be the \textbf{deployment of a healthcare management system}.

 \textbf{Evaluation} on another hand, corresponds to the fifth step, where the evaluation of the \textbf{artifact} takes place. In this phase, the product is evaluated. Rigor,effectiveness,efficiency and overall impact are considered, taking into consideration the problem and that the evaluation method may defer relatively to the nature of the given product. This step has the objective of measure if the \textbf{artifact} solves the problem and meets the objectives settled in the earlier stages. As an example, the \textbf{evaluation of the performance by tracking the key metrics} can be considered.

 At the realm of the sixth step, \textbf{Iteration and Refinement} is the step right after the evaluation. This is because, based on the observations, there may be areas where the \textbf{artifact} still lacks which compel for improvement. With this in mind, iteration is done and evaluation and demonstration occur multiple times until there insignificant improvement. As an objective, in this phase, there is the max refinement possible of the \textbf{artifact} recurring to iteration. To illustrate this, there is the possibility of image a case where \textbf{after testing, the system might be updated to include additional data fields or enhanced user interface features to improve usability}.

 Finally, in the sept and last step there is the \textbf{Communication} phase. Within this, the research findings are shared, artifact design and theoretical contributions are released and the form it takes are \textbf{academic papers},\textbf{presentations} and \textbf{reports to stakeholders}. Deconstructing this, here there is the documentation and dissemination of the findings, including both practical and theoretical contributions. As an example, there is the \textbf{Publishment of a paper describing some development in healthcare made by a given research}.

\subsubsection{Challenges in DSR}
Since this is a very important methodology within the scope of this project, it was very important to inspect probable challenges that could occur.
Despite powerful, \textbf{DSR} also comes with it's achiles heel's. By recognizing it's limitations, there is plenty of room to achieve that the outcomes are actual suficient to satisfy the need of practical and academical value. A set of challenges will be described below:
\paragraph{A. Balancing Rigor and Relevance}\mbox{}

\textbf{Balancing Rigor and Relevance} is the first probable challenge. Since this evolves a research, there are always 2 major risks to be shown: Presenting to much rigor and less practise or presenting to much practise and no theoretical presentation not soever. This becomes a problem because if doing research, the desirable outcome is to have both contributions and in case of to much rigor, there may be lack of practical implementation and, in another hand, to much practical could lead to few theoretical representations thereby no knowledge is passed. Reaching this balance imposes a burden but if managed harmoniously, it benefits not only the case under scrutiny but it also benefits others that may want to use the same idea in another context,influencing this way other sectors and gaining more recognition from the work.
\paragraph{B. Problem Complexity} \mbox{}

When it comes to \textbf{Problem Complexity}, \textbf{DSR} does go along with it but with extreme caution. This happens by virtue of sometimes becoming to hard to use, due to the hardness of understanding which problem is within a to broadly concept situation. Determining the appropriate scope of the problem is demanding and sometimes it can lead to a over complex artifact but there are countermeasures that can possible bring up a cure. This remedy can range from deconstruction the problem to high level representations but the idea here is to understand that this does come as a limitation of the methodology, thereby the need to address this challenge when a complex situation surges.

\paragraph{C. Artifact Evaluation} \mbox{}

\textbf{Evaluating} something is not always a smooth task, since sometimes even simpler tasks are a burden for consensus reach. With this in mind, \textbf{Artifact Evaluation} is considered also a challenge but even more in the context of various factors, such as environment detained of much intricacy's, vast variables and the incapacity of controlling external conditions. Also, when it comes to evaluate, there must be specified methods to do so, like mentioned before. This requires time and careful thinking from the team, creating burden and representing a hard obstacle. 

\paragraph{D. Iterative Nature of the Process}\mbox{}

As stated before, \textbf{Iteration} is a important characteristic of the \textbf{DSR} methodology. Thus, the normal workflow for which is, in this precise order, \textbf{artifact design} and \textbf{artifact evaluation} where this is looped all over again for refining. With this perspective in mind, several cycles occur and managing such extended process may become a objection to the project. In behalf of this, refining may represent messing with other existing parts of the \textbf{Artifact}, side effects that the management was not taking into account and also the possibility to not completely understand when the \textbf{artifact} is already good enough. By taking this into consideration, the team could understand when it is fine to left the loop, in which measure it is safe to implement features and what can be refined with effectiveness.

\paragraph{E. Generalization of Findings}\mbox{}

This challenge, comes from the need to dodge the generation of specific knowledge. This is imposed because, while you are conducting research, there is the objective of generating knowledge that can affect society and if this information becomes to much specific, it becomes harder to apply in various sectors having less relevance by not being applied to broader contexts. Ensuring that his does not happen and that the knowledge can be applied to various fields represents a major challenge.


\paragraph{F. Interdisciplinary Nature of DSR}\mbox{}

In some \textbf{DSR} projects,notably those exchanging interactions with sensitive spheres like healthcare, education or public safety, ethical and social matters can present challenges. Ensuring that the artifact aligns with ethical guidelines, respects privacy and evicts unintended social negative effects is critical but a burden.

\paragraph{G.  Ethical and Social Considerations}\mbox{}

\textbf{DSR} often requires knowledge from various spheres, particularly when corrective measures for solving complex and real-world complications is included. This vast sphere and integrative approach, while immeasurable valuable, can impose a barrier, as researchers need to develop different sides of view, methods, and frameworks which can represent to communication barriers or conflicting approaches.

\paragraph{I. Time and Resource Constraints}\mbox{}

\textbf{DSR} projects often mean long crafting into the design, development, and evaluation phases, all of which require compelling resources (time, funding, expertise). Atoning the requirement of a relevant and effective research process with the boundaries of project timelines and budgets can be a burden.

\subsubsection{Usage during this project}

The usage of this methodology will be more comprehensive during the second use case. This is because the first one is more theoretical and the second is more practical. Since this approach goes around creating knowledge through a practical multiple loop iteration, it does require that the project is dotted of such.

With this in mind and better discussed further, firstly, within this work there is a definition of objectives for this solution. Secondly, there is the design and development of the artifact, which is a blockchain network. Thirdly, this network passes by a demonstration to effectively see how it behaves. Forthly, evaluations around the solution are made, and in case the solution is already good enough, communication is made; otherwise, there is once again a process of iteration and refinement that starts once again in the demonstration, creating this in a way an infinite loop until the condition of the solution satisfying the project needs is made.

%
%
%    SWOT ANALYSIS
%
%
\subsection{SWOT analysis}
\textbf{SWOT analysis} is a strategic planning tool which is often related to business. This is because it is a simple and yet powerful overviewer, that empowers organizations to set a plan for that precise moment. The acronymom \textbf{SWOT}, stands in English for \textbf{Strengths}, \textbf{Weaknesses}, \textbf{Opportunities} and \textbf{Threats}, which represent the key factors that influence the organizations success or failure. If done multiple times and by several people, it can gain multiple new perspectives that help as a decision factor because it can probably give a deeper understanding of the company competitive position, identifying potential areas for growth and also empower the staff to think better in strategies to address their challenges.
The purpose is to provide a well structured framework for decision making, which accesses internal factors like strengths and weaknesses and also external factors like opportunities and threats.
Despite it's current usage being more directed to business, it also helps in other kind of projects such as personal career planning and even research. In this project there's the intention to use it. This is because the use cases are complex, having a tool like this can help others understand the current situation, can help the stakeholders of the research to work according to the books and it can give a hint to those interested on the project to discover if it is doing well or not. 
With this in mind, there is detailed information about it further, explaining each particularity of this valuable tool.

\subsubsection{Purpose of SWOT Analysis}\mbox{}\\
The purpose of \textbf{SWOT analysis} is to serve as a decision-factor for organizations. This is done by analysing internal and external factors, providing a clean environment to understand the organization competitive position and it's ability to achieve it's objectives. Putting it in more deep terms, the key purposes include:

\paragraph{A. Strategic Planning}\mbox{}\\
To help developing strategies where the strengths will have more emphasis, weaknesses will be strengthened, opportunities exploiting will be taken and threats will be mitigated. This can be done if the organization aligns their internal strengths and weaknesses with external opportunities and threats.

\paragraph{B. Identifying Competitive Advantage}\mbox{}\\
To outperform competitors, the organization must focus on areas where they have clear advantage. This is done by emphasizing internal strengths, such as capabilities,resources and unique advantages.

\paragraph{C. Addressing Weaknesses}\mbox{}\\
For leaders to address challenges and reduce potential risks, \textbf{SWOT} offers insights about where the organization is vulnerable.

\paragraph{D. Capitalizing on Opportunities}\mbox{}\\
For the recognizing and to take advantage of the market trends,technology advancements and emerging markets that could be beneficial for the growth and innovation of the organization, \textbf{SWOT} can identify the external opportunities.

\paragraph{E. Mitigating Risks}\mbox{}\\
In order to offer risk mitigation that negatively affects the impact performance of the organization, \textbf{SWOT} offers a way to identify external threats, such as competition,regulatory changes or macro-economic downturns. This enables careful thinking about strategies that must be taken.

\paragraph{F. Informed Decision-Making}\mbox{}\\
For developing new strategies, it is important that there is plenty of knowledge about all aspects of the internal and external environment. \textbf{SWOT} helps in this matters because, it makes sure that all aspects are considered, therefore this creates more error prone new initiatives or facilitates when it comes to enter new markets.


\subsubsection{Internal Factors: Strengths and Weaknesses}
Representing the upper part of the \textbf{SWOT} standard visualization, there is the internal factors \textbf{Strengths} and \textbf{Weaknesses}. This is what influences the most, the ability to the achievement of the organization objectives. Understanding both strengths and weaknesses is crucial as a building block for enhancing existing capabilities and check out areas where the organization usually outperforms or the opposite. To put in other terms, the organization has the capability for control and actively manage this two, in a way where the focus is to improve but the decisions to do so are not always deterministic into that direction. With this in mind, let's make a deep dive into each to understand it further:

\paragraph{A. Strengths}\mbox{}\\
\textbf{Strengths} are the internal contributions, so that the organization has a certain number of attributes,resources and capabilities for enterprise level competitive advantage. These are the areas or aspects where the company can outperform others, that can be used to collect opportunities and even defend against possible threats. Identifying strengths is something very valuable for organizations, because it helps in focusing on the best attributes of the organization, thereby fixing their market position.
As an example of Strengths, it could be something like: \textbf{Strong brand Reputation}, where a well established brand is widely recognized and trusted by consumers. \textbf{Skilled Workforce}, in cases where there is a qualified and experienced team who drive innovation and efficiency. \textbf{Proprietary Technology}, which are unique or patented technologies that provide a competitive edge in the market. \textbf{Operational Efficiency}, for processes and effective cost management that lead to higher profitability. \textbf{Customer Loyalty}, a loyal customer base that provides consistent revenue and positive word-of-mouth marketing and finally, more concretely for research, \textbf{Having more than enough resources}, which stands for having such resources that makes the research something more than feasible which usually is a huge challenge.
By levering this strengths, an organization can maintain or even enhance it's market position, thereby maximize its competitive advantages in relation to others, improving by extension its overall economic performance.

\paragraph{B. Weaknesses}\mbox{}\\
\textbf{Weaknesses} are the core stone of the internal given limitations, for areas where the organization actually underperforms relatively to competitors which by extension can prevent the organization from achieving it's objectives, by representing a obstacle to the capitalization of the existing opportunities and even  by exposing the company to risks presented in external threats. \textbf{Weaknesses}, if spotted correctly, allow the organization to take measures for correctness and minimization of the impact that this unfortunate characteristics may pose in future iterations. Addressing this is essential for vulnerabilities mitigation, improve performance and prevent competitors from exploiting these causalities.
As an example, there is a lot of thinks that can be mentioned like: \textbf{Outdated Technology}, because by relying in older systems or tech can slow down processes,limit capabilities and even possibly increase costs for maintenance. \textbf{Limited Financial Resources}, where insufficient capital to invest can pose a problem to initiatives,R\&D and further expansion if desired. \textbf{Inefficient Processes}, in the idea of resource wasting, higher costs resulted from activities and delays in delivery. \textbf{Weak Brand Presence}, which represent a weakness in the essence of reduced market share and finally \textbf{High Employee Turnover}, where talent within the company is hard to maintain, thereby results in higher recruitment and training costs.

\paragraph{C. Both combined}\mbox{}\\
By understanding and analysing \textbf{Strengths} and \textbf{Weaknesses}, there is the possibility of creating a good synergy between both which can result in organizations positioning themselves to achieve their goals. \textbf{Strengths} help creating and sustaining competitive advantages and \textbf{Weaknesses} ensures that there are no barriers for further success. In other words, considering both interchangeably will imply future success, therefore it is very important to careful think in all of those and afterwards try to see it's relations.


\subsubsection{External Factors: Opportunities and Threats}
When it comes to external factors, there should be informed that those factors are the ones that the enterprise cannot control, since they don't own them and they are external to their direct manage. This means that despite the organization not being able to coordinate them, they can still take actions to make sure that those causalities don't affect them that much or that don't affect them in any way. These aspects are \textbf{Opportunities} and \textbf{Threats} and they play a huge paper in the \textbf{SWOT analysis}. Putting in different terms, those are the existences that are based in outside situations that are beyond their control, such as market trends, macro-economic indicators, competition and others. \textbf{Opportunities} provide a path for growth, while \textbf{Threats} present a potential challenge that could negatively impact the company.
A deep dive into each factor will be done in this section:

\paragraph{A. Opportunities}\mbox{}\\
By identifying opportunities, organizations are doted of taking proactive steps toward expanding their market presence or even aggregating more into their competitive deck of cards. To achieve this, the organizations must attend to their external circumstances in order to effectively search over exploits that they can take into accordance to represent a new advantage. These, if used correctly, can help the organization grow, innovate and improve performance.
As an example, there is the following possible aspects: \textbf{Emerging Markets}, which stands for entering into a new geographic or demographic market, that offers potential for growth and expansion. \textbf{Technological Advancements}, that leverage new technologies that improve efficiency, number of product offerings or new innovative services.\textbf{Regulatory Changes}, related to new policies or regulatory shifts that if,taken into favor, can benefit substantially the organization in scope. \textbf{Shifts in Consumer Preferences}, that aims to answer to new consumer and trend behavior that can be targeted by a new product that may be on release and finally \textbf{Strategic Partnerships}, where forming alliances and collaborations with other companies can open new revenue streams.
By paying attention to opportunities, organizations can gain a competitive advantage, enter new markets and drive long-term growth.

\paragraph{B. Threats}\mbox{}\\
\textbf{Threats} on the other hand, are challenges or risks that could affect negatively the organization. Some of these are expected, but the worst ones are those that no one is expecting and because of this there must be a lot of thoughts about it, because a lot of this comes from disguised situations. Being able to spot such things, is a important skill when it comes to project maintainability and therefore it is specially important to pay attention to, because it can be the meaning of the project ceasing and catching them early can help to take preventive measures that could minimize or even remove the effects from it. Due to the external nature, the enterprise has low control over this kind of situations but being interested in such matters, can reduce significantly the downturns.
As an example, threats can be represent as \textbf{Increased Competition}, where the increase of offering of similar products can potentially remove some market share. \textbf{Macro-economic downturns}, where the economy slows down for some reason which results in less demand for the project. \textbf{Regulatory changes}, which can result in the enterprise suffering by politics that may be against it or that revoke previous given benefits. \textbf{Changing Consumer Preferences}, that happens when the consumer shifts it's interest in the current product and benefits the competitor leaving the organization behind and \textbf{Technological Disruptions} where the advances in technology actually removes the need that the enterprise was serving, therefore making the organization service/product obsolete.
In case well informed, organizations can adapt and overcome this challenges, by preparing defensive strategies that will mitigate risks,ensure business continuity and market share protection.

\subsubsection{How to Conduct a SWOT Analysis}
In order to come up with a \textbf{SWOT Analysis}, a systematic process of evaluating the internal and external factors is required. In other terms, the \textbf{Strengths},\textbf{Weaknesses},\textbf{Opportunities} and \textbf{Threats} are something that is in constant change, because there may be variables that were not putted into custody, new perspectives and internally or externally there is always something that may be changing because a organization is something dynamic. 
With this in mid, here's a comprehensive step-by-step guide to come up with a \textbf{SWOT}:  

\paragraph{A. Define the Objective}\mbox{}\\
As a first step, a thematic should be specified. This makes sense since a organization is a very complex environment, thus there is a need to specify a specific matter within the company like a specific project,business decision,product launch,market entry or a specific strategic plan.

\paragraph{B. Gather Relevant Data}\mbox{}\\
Moving to the second step, there should be collected the most data possible for gather the most important internal and external factors. These factors, based on the data can be collected through using financial reports, employee feedback and operational metrics in the case of the internal and market research,industry trends and competitor analysis in the case of the external. The data sources range from financial performance,operational efficiency,staff skills,technology and others in the case of the internal and market trends,competitor behavior, macro-economic events and customer preferences in the case of the external.

\paragraph{C. Identify Strengths}\mbox{}\\
As a forth step, there must be a focusing in spotting the strengths. To actually get to the point where the organization can extract these strengths, a certain number of questions should be answered to carefully revise our perspectives:
\begin{enumerate}
    \item What does the organization do well?
    \item What are our unique resources or capabilities?
    \item What advantages do we have when compared to the competition?
\end{enumerate}
By answering this questions, there must be already sufficient ground to come up with actual valid answers.

\paragraph{D. Identify Weaknesses}\mbox{}\\
In the fifth step, internal weaknesses are revised. Like mentioned before, recognizing them it's very important as other aspect of the \textbf{SWOT} but, like the threats, sometimes this weaknesses are not that easy to spot and lack of self feedback should be a barrier to come up with answers. Alike the previous step, there are questions that can help with this, being that questions the following:
\begin{enumerate}
    \item What are the areas where we need improvement?
    \item What resources are we lacking?
    \item What processes are inefficient?
\end{enumerate}
Like the previous step, the same applies when it comes to questions. By answering them there should be support enough to discussion.

\paragraph{E. Identify Opportunities}\mbox{}\\
As a sixth step, now in the external realm, there must be answers relative to the opportunities. Likewise, but now for growth,innovation and performance exploitation, there are also a certain number of questions that could make the answer easier:
\begin{enumerate}
    \item What external trends can we capitalize on?
    \item Are there any upcoming regulations or technological advances we can take advantage of?
    \item What market gaps or consumer needs can we address?
\end{enumerate}

\paragraph{F. Identify Threats}\mbox{}\\
In the realm of the septh step, on the other hand it comes the threats, another external factor but related to risks. These may include competitive pressures, macro-economic downturns, changes in regulations and others. Likewise again, there are a set of questions that could help with this:
\begin{enumerate}
    \item What obstacles do we face?
    \item Are there external factors that could cause problems for our business?
    \item What are our competitors doing that could threaten our market position?
\end{enumerate}

\paragraph{G. Analyze the Results}\mbox{}\\
In the eight step and ,now that all the results were reunited, there's is plenty of capabilities to make a structured analysis. This analysis is precisely to make a correlation between all of the aspects and ,this way, play around with what could be strategies that could help in the given areas. It is important to mention that because they are related to which other, what could improve one side could make the other worst, which means that in the same proportion what could be good can actually have side effects that the organization cannot berry. 
What a organization must do in this kind of situation is: \textbf{understand how internal strengths can help on gathering opportunities} and \textbf{understand how weaknesses might expose the organization to threats}. As an example of this, in the case of the strengths to exploit opportunities there is the usage of the strong brand to expand to new markets. In the case of the weaknesses to avoid threats, in the other hand there is the update of technology to mitigate the risk of competitors offering more advanced options.

\paragraph{H. Develop Strategic Actions}\mbox{}\\
In respect to the step eighth, this is the part where there is a creation of a given set of strategies for each element of the \textbf{SWOT}. This is challenging because they are correlated, which creates fuzz but by thinking on the effects of this strategies, bad side effects can be avoided or atleast can be taken in account.

\paragraph{I. Review and Update Regularly}\mbox{}\\
Finally in ninth, there is a comprehensive review of what has been done so far, contributing to what was mention before with the constant change of these aspects. Either because your strategies are already making effect or simply because of the dynamic nature of a organization but the main idea here is that the \textbf{SWOT} will be continuous updated.

\subsubsection{Applications of SWOT Analysis}\mbox{}\\
\textbf{SWOT} analysis is something that can be applied across various fields, because of it's ease of using and versatility. With this in mind, in this section, it will be delving into some of those sectors for understanding until each measure can this be useful.
\subparagraph{A. Business Strategy and Planning}\mbox{}\\
To achieve long-term objectives,grow market share and enhance competitiveness, there is a widely usage of \textbf{SWOT} by the businesses by developing and refining their strategic plans based on this. It's application ranges from \textbf{Identifying strengths to levare in competitive markets},\textbf{Addressing weaknesses to improve operations or product oferring},\textbf{Spotting opportunities for market expansion or new product development} to \textbf{Recognizing threats from competitors or economic shifts}.

\paragraph{B. Product Development and Innovation}\mbox{}\\
To respond to shifts in customer preferences, improve existing products and introduce new products or services, the usage of \textbf{SWOT} is also relevant. It's application ranges from \textbf{Identifying product strengths (e.g., unique features, superior quality) that can differentiate it in the market},\textbf{Highlighting weaknesses such as high production costs or design flaws},\textbf{Spotting opportunities like untapped customer segments or technological innovations} to \textbf{Evaluating potential threats from competing products or changing customer demands}.
\paragraph{C. Marketing and Brand Positioning}\mbox{}\\
To reaching target audiences,develop strategies and understand the organization brand is being positioned in the market there is also the respective usage of \textbf{SWOT}. Some of its applications in this sector are \textbf{Analyzing brand strengths such as strong customer loyalty or a high-quality product line},\textbf{Addressing marketing weaknesses, like lack of digital presence or poor brand awareness},\textbf{Identifying new marketing opportunities through social media platforms or influencer partnerships} and \textbf{Anticipating threats from new competitors or shifts in consumer preferences}.
\paragraph{D. Competitive Analysis}\mbox{}\\
\textbf{SWOT} can be also used to comparison with other organizations. With tangible characteristics shared among them, there is the possibility to identify competitive advantages and potential areas where competitors are a actual threat to the business. It's applications are enormous in this sector like \textbf{Assessing the company's strengths relative to competitors},\textbf{Identifying weaknesses that competitors may exploit},\textbf{Evaluating opportunities in the market that competitors have not yet pursued} and \textbf{Identifying threats posed by existing or emerging competitors}.
\paragraph{E. Project Planning and Management}\mbox{}\\
\textbf{SWOT} can also become a huge allie when it comes to project planning and management. This is because, it ensures that projects are well prepared to succeed by knowing the internal resources and external risks. More concrete applications of this are situations like \textbf{Leveraging strengths like skilled teams or efficient workflows to meet project objectives},\textbf{Identifying internal weaknesses such as limited budgets or insufficient manpower that could affect the project timeline},\textbf{Recognizing opportunities, such as new technologies or favorable market conditions that can enhance project success} and \textbf{Anticipating threats such as resource shortages, regulatory hurdles, or changing customer demands that may derail the project}.
\paragraph{F. Risk Management}\mbox{}\\
To mitigate risks more effectively by identifying both internal weaknesses and external threats, \textbf{SWOT} can also become a good allie within this matter. This is because it identifies and addresses potential risks. 
Applications on this matter using this tool are something like \textbf{Identifying weaknesses that might expose the organization to financial or operational risks},\textbf{Analyzing external threats such as economic downturns, supply chain disruptions, or technological obsolescence},\textbf{Using strengths to build resilience against risks} and \textbf{Developing contingency plans to mitigate identified risks}.

\paragraph{G. Personal Development and Career Planning}\mbox{}\\
In case a personal development plan is something desired, \textbf{SWOT} is a good tool to spot threats to make good career choices by spotting strengths, weaknesses and even opportunities. This means that this tool is flexible, until the point that it is not applied only to organizations but very relevant for other types of cases. Some of the applications within this concrete case are something like \textbf{Identifying personal strengths such as skills, qualifications, or experiences},\textbf{Addressing weaknesses such as skill gaps or limited experience},\textbf{Recognizing opportunities like new job openings, networking possibilities, or educational opportunities} and \textbf{Evaluating external threats like industry changes, job market competition, or economic uncertainty}.

\paragraph{H. Nonprofit and Social Sector Strategy}\mbox{}\\
For identification of sustainability and impact opportunities and to deliver their mission, nonprofit organizatons and social enterprises also use \textbf{SWOT}. How it is applied is listed from \textbf{Evaluating internal strengths such as dedicated volunteers or unique fundraising channels},\textbf{Identifying weaknesses like limited financial resources or operational inefficiencies},\textbf{Spotting opportunities such as emerging social trends or partnerships with other organizations} to \textbf{Addressing threats such as changes in public policy, donor fatigue, or increased competition for funding}.

\subsubsection{6. Advantages of SWOT Analysis}\mbox{}\\
There was been a lot of \textbf{SWOT} coverage so far. Like the purpose,it's factors,how to actually use it and its major applications. With this in mind, there will be also the coverage of its advantages in a more direct way so there is comprehensive understandability about the meaning of using such tool. Without further delay, there will be a coverage of the main ones below:

\paragraph{A. Simplicity and Ease of Use}\mbox{}\\
The first advantage is it's ease to use, which is no surprise due to its straightforwardness. It is simple to understand therefore simple to use, making it feasible for a range of users like businesses from small to big and even for individuals.

\paragraph{B. Comprehensive Overview }\mbox{}\\
The dual approach, internal and external section of factors,  ensures that, in a holistic way, there is coverage of all critical aspects for decision making. This is a key factor for making decisions since it gives less burden when analysing for coming up with a vision of the current state of a given project.

\paragraph{C. Encourages Strategic Thinking}\mbox{}\\
\textbf{SWOT} creates a environment for strategic system, because it requires the organization to constantly change the aspects they have been putting into it. By constant changing, this obligates management to look beyond day-to-day operations and think more in the broader picture, focusing on long-term goals and strategies.

\paragraph{D. Identifies Core Competencies and Areas for Improvement}\mbox{}\\
By identifying strengths and weaknesses, it helps organizations recognize their key competences and areas where they can do better, specially within the internal realm since the external realm can only be tangible through taking action from inside. This conscious, leads to a obligation for the management side to take more focused strategic decisions.

\paragraph{E. Supports Informed Decision-Making}\mbox{}\\
For supporting data-driven decision-making, \textbf{SWOT} is widely used for revealing key insights about the organization position and the surrounding environment. This is essential for a organization to take well-informed decisions by having in their possess structured assessment of internal and external factors.

\paragraph{F. Flexible and Versatile}\mbox{}\\
It has been proven that \textbf{SWOT} is flexible and versatile, because has it was mention before, it can be applied to a vast numerous of cases like business planning,product development,marketing,project management,risk assessment,personal career planning and even nonprofit strategy development.

\paragraph{G. Fosters Collaboration and Team Input}\mbox{}\\
Since the data is kind of bias, having different perspectives can be a valuable insight for the organization and this is what creates a environment for comprehensive collaboration and input from the staff. By involving multiple people across the organization, \textbf{SWOT} reunites all of this insights and it helps in covering all the aspects for a well-rounded strategy for the future.

\paragraph{H. Promotes Strategic Alignment}\mbox{}\\
Since after the last step, there is continuous monitoring of the situation and by extension mutation of the \textbf{SWOT}, alignment is expected to occur, because by having the internal and external aspects a well-coordinated strategy gets formed,  thus all get to work into the same direction.

\paragraph{I. Risk Identification and Mitigation}\mbox{}\\
Another advantage of the \textbf{SWOT} is the prior risk detection. This is very important, due to the fact that if there are notices of deviation, organizations will develop strategies that will mitigate those risks, thereby making them prepared for challenges and uncertainties.

\paragraph{J. Cost-Effective and Efficient}\mbox{}\\
\textbf{SWOT} is cost-effective method for evaluating business environments. This is evident, due to the fact that this analysis don't require significant financial investments or sophisticated tools like other approaches. Putting in another words, it is not expensive to use this approach and it does serve its purpose which is perform a quick and yet effective analysis of the current situation of the organization, creating means for further decision making.

\subsubsection{Limitations of SWOT Analysis}\mbox{}\\
While \textbf{SWOT analysis} is a valuable strategic tool, it has several limitations that can affect the effectiveness of the insights. By getting to know the limitations that this tool offers, it is possible for us to delve into the realm of contra-measurements or good practises that in return can help to avoid this nuances and make a fair analyse. After deep thinking, there is below a coverage of the most relevant limitations that this framework can find:

\paragraph{A. Lack of Prioritization}\mbox{}\\
Altought this tool reunites all of the most important information, it does not offer a structured way to prioritize them. Without a clear ranking of which factors are most critical, it becomes fuzzy for the management to take action due to the struggle of knowing which issues should be taken into custody first.

\paragraph{B. Subjectivity}\mbox{}\\
\textbf{SWOT analysis} is unfortunately very subjective, as it relies in perspectives and opinions, which leads to a analysis. This can introduce bias or incomplete perspectives depending of the personal, leading to a vision that may not be the true representation of the organization situation. 

\paragraph{C. Over-Simplification}\mbox{}\\
Simplicity is seen as a advantage in a lot of situations, but this becomes a burden in more complex situations where factors may not fit in the given categorizations or ,even worst, they may be a strength and a weakness at the same time.

\paragraph{D. Static Snapshot}\mbox{}\\
Despite the \textbf{SWOT} analysis being updated frequently, they become quickly outdated in case the organization is to much dynamic, therefore, actions must be taken in order to make sure that this remains updated which is very difficult in those kind of situations.

\paragraph{E. No Quantitative Data}\mbox{}\\
Another limitation of this tool, is the fact that the data is largely qualitative, which means that it does not contain hard data or any kind of metrics. This lack of quantitative data makes it difficult to measure significant in some points and also even more harder to establish goals which usually are better with numbers.

\paragraph{F. Limited Actionable Insights}\mbox{}\\
\textbf{SWOT} reveals itself as limited in the actions to take, this is because it only presents a analysis and not any kind of recommendation regarding actions to take over certain situations. In other terms, the tool is limited into giving insights of the action to take in facing common problems, which means the most it can do is simply present a simple a analysis.

\subsection{Risk List}

In the following table, the risks that directly or indirectly influenced the development of this dissertation are listed. Depending on the identified risk, a mitigating action is proposed in order to minimize the impact of the risk. The scale for the Probability and Impact columns is measured from 1 to 5, where 1 corresponds to a very low risk and 5 to a very high risk.

\begin{longtable}{|c|p{2cm}|c|c|c|p{2cm}|c|} 
    
    \hline
     \textbf{N} & 
     \textbf{Description} & 
     \textbf{Probability(P)} & 
     \textbf{Impact(I)} & 
     \textbf{Seriety(P*I)} & 
     \textbf{Mitigation Action} & 
     \textbf{Verified}  
     \\ \hline
    \endfirsthead

    \hline
    \textbf{1} 
    & 
    Unreachable deadlines
    & 3 
    & 5 
    & 15 
    & Planning better the next events and gain experience from previous deliveries;
    & Yes 
    \\ \hline
    
    \textbf{2} 
    & 
    Requirements of the project change
    & 2 
    & 4 
    & 8 
    & Make frequent communication with stakeholder;Check well the limitations imposed by the project;
    & No
    \\ \hline
	
    \textbf{3} 
    & 
    Lack of Time
    & 4 
    & 5 
    & 20 
    & Implement only what is actually needed
    & Yes
    \\ \hline

     \textbf{4} 
    & 
    Resources
    & 4 
    & 5 
    & 20 
    & There are no mitigation measures
    & Yes
    \\ \hline

     \textbf{5} 
    & 
    Incorrect understanding of data
    & 2 
    & 5 
    & 10 
    & Spend more time analyzing the data; Make sure that the data is relevant for the object of study;
    & Yes
    \\ \hline
    \end{longtable}
    
    
    
    \begin{longtable}{|c|p{2cm}|c|c|c|p{2cm}|c|} 
        \hline
     \textbf{N} & 
     \textbf{Description} & 
     \textbf{Probability(P)} & 
     \textbf{Impact(I)} & 
     \textbf{Seriety(P*I)} & 
     \textbf{Mitigation Action} & 
     \textbf{Verified}  
     \\ \hline
    \endfirsthead
    \textbf{6} 
    & 
    Mistakes
    & 1 
    & 5 
    & 5 
    & Don't do tasks in automate mode; Focus more in the tasks;
    & Yes
    \\ \hline

    \textbf{6} 
    & 
    Security implications
    & 1 
    & 5 
    & 5 
    & Follow standards; Try to learn more as the time passes;
    & No
    \\ \hline

    \textbf{7} 
    & 
    Complexity of the project
    & 2 
    & 3 
    & 6 
    & Document Everything; Simplify everything;Join Communities;
    & Yes
    \\ \hline

    \textbf{8} 
    & 
    Unknown Errors
    & 2 
    & 4 
    & 8 
    & Join Communities;Arrange more debug mechanisms;
    & Yes
    \\ \hline

    \textbf{9} 
    & 
    Acceptance
    & 4 
    & 5 
    & 5 
    & Make the best solution possible;Make clear the need of the solution;
    & No
    \\ \hline

    \caption{Risk List: General} \label{tab:activity_schedule} 

\end{longtable}

\subsection{Relevant tools}

Before delving into the use cases and what was done in practical terms, there is a need to explain seamlessly which tools were used during the project. This is because, by explaining what was used, by extension, an assumption by the reader for the reason why certain things were inserted in the project is achieved. Some tools are related to blockchain, since that is the main subject under scrutiny along with the healthcare sector, but others, on the other hand, are extensions of a product that is strong by itself. Reasons for the usage of blockchain were already mentioned in the State of Art; thereby, there is no reason to go much deeper in this aspect, but returning to the other tools, they are needed for commodity purposes, and probable risks resulted by the usage of certain tools will be covered further in this thesis. Also, it is important to mention that the idea around this project is not to achieve excellence in decentralization, to simply use emergent technologies because they are trendy, but to try to solve the existing problems with the best set of tools that are proven to be the best in the current market and perhaps standardize the way private blockchains are designed to gain broad acceptance. Standardization helps with broad acceptance because this way implementations are not that fuzzy like they were before, contributing to ease in implementations and creating a common path between people that, in case of obstacles, there may be a current solution already because someone crossed the same path before. This is essential for a vast spread adoption, and thereby that's our contribution here.
There are 3 covered sections within the tools; the list is not at all complete, but there is an effort to cover the most important concepts of technologies that were used during the project. This will empower the reader with the correct view of what has been done so far.
There is a Microservices section in which the tools are a part of the other tools that will be covered. They retain an honored representation on this project because their usage is meant to empower administrators with capabilities such as resource management, easier setup of networks, decoupled services, scalability, and high availability.
Another important section is analytics; this is very important for any type of robust system, and its aim does not reside only in maintainability. It empowers the project with also mechanisms to detect bottlenecks, detect unexpected behavior, troubleshooting support, debug support, benchmarking, observability, measures to better resource management, and more.

Finally, there is also the web development section, which is used in this project as the core to build extra services that may be required for the normal function of the network. They work mostly for communication support, for ensuring security, and to help automate procedures during the results obtained in this research, but also for future usage while using this in a real environment.


\subsubsection{Blockchain}
This section will contain all necessary related or close related information about blockchain that is being used in this research. The aim in this section is to give a good fundamental overview about \textbf{Hyperledger Fabric}, the private blockchain technology that is used all over the investigation and also to give some hints about \textbf{IPFS}, which despite not categorized as a blockchain, has a lot of characteristics that resemble a blockchain thereby despite the section name, it still makes sense to put it here for the purely sake of relation between the concepts making it easier to digest. The first is considered a general purpose technology, which can be used with all kinds of data but having the capability of working with all kinds of data and at the same time be capable of dealing with all types in a effective way that's a completely different story and, because of this and also to respect to files, there was a need to think in a extension of the blockchain capabilities where \textbf{IPFS} can effectively play its rule. Altought intriguing, in this work there was not a measurement of performance regarding this technology and how it would behave in the private landscape, which means that atleast in this realm there are no tangible evidence of effectiveness on our side, which may result in the future measurements while working interchangeably with both the blockchain and this technology for storing files, exacly like describe in one of our future mentioned use cases.

\paragraph{3.5.1.1- Hyperledger Fabric}\mbox{}\\
To understand \textbf{HLF} (Hyperledger Fabric), there must be an understandable view of what a blockchain is. In an abstract way, this concept refers to an immutable ledger maintained within a distributed network of peer nodes. Each node maintains a copy of the ledger by applying the same logic to their ledger. If all reach the same result, this view of the world becomes valid, and everyone gets to keep the same state. The process mentioned here is thereby called \textbf{consensus}, which is a protocol where there is a validation of blocks that inside contain these tasks that must be endorsed by a collective number of nodes. To actually reach the point where the block is validated, all of it's transactions must be accepted by a number of nodes that depends on the configuration of the network. If a transaction gets no validation, this transaction is not included in the state of the ledger, which means it is not valid.

The first and most widely used cases of this are \textbf{Bitcoin} and \textbf{Ethereum}. Both serve different but related purposes; both fall into the class of blockchain, but these ones are public ones, like mentioned in the state of the art of this thesis.

As the popularity of these 2 plumed, other derived technologies coming from the same concept started to emerge. With this in mind, a more enterprise-friendly approach started to grow, though many organizations still need performance characteristics that presently permissionless blockchain technologies are unable to satisfy. Also, there are other requirements that also present a challenge for this approach, which are: the requirement of identity for financial transactions where \textbf{Know-Your-Customer} (KYC) is required and \textbf{Anti-Money Laundering} (AML) regulations; networks need to be permissioned; high transaction throughput performance; low latency of transaction confirmation; privacy; and confidentiality. \textbf{Hyperledger} was literally designed to meet these precise characteristics, not needing to adapt like other solutions, and that's what differentiates it from others.

\textbf{Fabric} is an open source permissioned distributed ledger platform designed for use in enterprise contexts, delivering a certain number of capabilities that are imperative in business scenarios. It is maintained by the \textbf{Linux Foundation}, and it presents an architecture that is modular and also configurable. It supports smart contracts through general purpose programming languages like \textbf{Go}, is permissioned, has pluggable consensus protocols that range from existing default ones to custom, does not require a native cryptocurrency, and offers one of the better performing platforms today of this kind for transaction processing and transaction confirmation latency.

This framework will be intensively used in the provided use cases. With this in mind, this section will be a solid coverage all over the main concepts of this framework.

Relatively to \textbf{Permissioned Networks}, this is a concept relative to a type of blockchain. Dissimilar to public blockchains such as \textbf{Bitcoin} and \textbf{Ethereum}, often called \textbf{permissionless} blockchains, these ones restrict access to a set of known participants, making them ideal for applications that require trust and control over the membership. Like mentioned before, \textbf{Hyperledger fabric} is one of these types, and so is \textbf{Corda}. Both platforms emphasize accountability without relying on a central authority, enabling organizations to construct custom and heterogeneous applications where there is a clear conscience of who can participate in the network \cite{permissioned-blockchains}. Usually, the usage of one restricts the usage of the other.However, there are also approaches where networks may want both \textbf{permissionless} and \textbf{permissioned}, referred to as \textbf{two-tier blockchain}, where both types are combined to allow both participation and validation, such as supply chain or cross-border financial transactions \cite{two-tier-permission-blockchains}. For the context of healthcare, \textbf{permissioned} is obviously more appealing, such that \textbf{operational efficiency} and \textbf{transaction security} when combined with restricted trust between participants is needed for \textbf{security} and data \textbf{privacy} \cite{permissioned-vs-permissionless-tradeoffs}.

Furthermore, there is also the modularity of \textbf{Hyperledger fabric} to be covered. It has 5 components: the \textbf{Peer},, \textbf{Orderer}, \textbf{Chaincode}, \textbf{Ledger}, \textbf{CA's (Central Authorities)}, and \textbf{client}. Each component will be covered with more extension in this category.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/tools/hyperledger/hlf-minimal-setup.png} % Change this to your image file
    \caption{Major components Relationship}
    \label{fig:sample-image} 
\end{figure}

As an overview above, there is an image that illustrates not the communications but how components may relate with each other. Also, it should be noted that this is very far from representing a whole system, where usually multiple peers must endorse transactions and multiple orderers must order transactions and blocks. Putting in another terms, the representation here explained is just for the sake of understanding how individual components work with each other. With this in mind, each component's individual functions will be covered more ahead.

The \textbf{Peer} (orgx-peer1) in this representation is the middle point of everything. This is because it is the one that receives the transaction proposals and endorsements that come from other peers by previous request of a client. It also receives blocks from the \textbf{orderer}, and it is responsible for endorsement of transactions by using the \textbf{chaincode}, and furthermore, after validation through this \textbf{endorsements}, it is the one that receives a \text{block} containing valid transactions from the \text{orderer} (orgx-orderer), for then to include in it's \text{ledger} (orgx-couch-peer1). The \textbf{client}, on the other hand, manifests in two ways: the client itself as an actor and the \textbf{client-api}, which turns out to be the application itself that is responsible for calling the \textbf{Peer}. The \textbf{chaincode} (chaincode-as-service) simply runs logic, which is what the \textbf{Peer} endorses, by running the application against the \textbf{ledger} (orgx-couch-peer1). In respect to the \textbf{ledger}, it is used as a mock to endorse transactions to see if the output is the same within the vast majority of peers.The \textbf{orderer} receives transactions and puts them inside of blocks, and after ordering these transactions and blocks, it delivers the blocks to the peers. Finally, the \textbf{CA}(certificate central authority) only gives cryptographic materials so that components can identify themselves within the network. It must be known as well that the chaincode-as-a-service has the option to create TLS connections with the peer. In this picture there is only one CA for purposes of simplicity on this explanation, because usually there can be a chain of CA's and in each level, for purposes of good practices, should have 2 CA's where one is for normal certificates and the other is for TLS. The difference between normal and TLS certificates is that one is for identification and the other is to secure the communications with encryption. Once again, in this representation only normal certificates are considered, but within the network there are both, which contribute to a secure environment. \cite{rfc5280}.Now that their relations were described, let's describe them in more deep terms for further explanation.

In what concerns the \textbf{peer}, this is a fundamental concept within the network since it is responsible for managing \textbf{ledgers}, \textbf{smart contracts}, \textbf{transaction proposals}, and \textbf{endorsements}. \textbf{Ledgers} and \textbf{Smart contracts} will be explained further, but a transaction proposal is a transaction that was signed by some entity that wants to be further endorsed by the peers for then to be included in the ledger if valid. \textbf{Endorsements}, on the other hand, are the process of a peer running the logic within the proposal against its ledger. Depending on the configuration of the channel and the number of peers that validated this proposal, this transaction will be included on the block or not (within the orderer side); therefore, it could be or not included in the ledger depending on its validity.

In the \textbf{Orderer} sphere, however, there is the responsibility of receiving endorsed transactions, which, depending on the number of signatures of the peers in it, could add the transaction to a block or reject it. In the case of the first, after ordering the transactions and the given blocks, it returns to all the peers this given block. Block this that will be used for the peer to insert its operations against its ledger, therefore ensuring that every single peer has the same view of the state because they are all running the same operations against the same state. To achieve this, the orderer uses what is called a \textbf{consensus protocol}. This protocol is a mechanism to reach agreement in the sequence of transactions, but that will be discussed further.

Another vital pillar for the network is the \textbf{Smart contracts}, more commonly called \textbf{Chaincode}. This is where all of the business logic runs, and it uses abstractions to communicate with the ledger. Usually, installation is required only once. However, it must be added per channel if that is the intention of the participants within that channel. This means that I can have the chaincode installed, but this chaincode could potentially not be allowed to be called because, for that, it must be proposed to be added and committed when a certain number of participants agree on its insertion. Besides that, there are 2 typologies for usage of chaincode: \textbf{normal chaincode installation}, which occurs per peer and the chaincode becomes more like a side container, and \textbf{chaincode as a service}, where the chaincode becomes a service and to install it there must be passed metadata for the connection management. The normal way is often used for questions of ease, and, along with it, there is the certainty that the chaincode that is being invoked is in fact the desired one, since it is literally a side container or side service of the peer, therefore also giving more isolation to it. The other way, however, depending on how it is deployed, it can be reused by multiple peers, which means that if an operator does not retain this service within its infrastructure, it may represent a risk since the endpoint could be the same and its application could be changed. Be mindful that, in case the \textbf{chaincode as a service} is adopted, depending on how it is implemented, it can be exposed to the common microservices vulnerabilities, and further actions should be taken to prevent these \cite{microservices-common-vulnerabilities}.

\textbf{Ledger}, on the other hand, is a normal database like \textbf{couchdb} \cite{couchdb}. This is the state that the \textbf{Peer} uses as a mock to run its business logic and get an output for later usage for the endorsement procedure. Transactions can be rejected by either violating some business logic or by presenting different outputs among the peers. As an example, let's suppose there are 3 peers. In the first situation, peer 1 rejects the transaction because the business logic says so. The other 2 peers, peer2 and peer3, respectively, accept the transaction. In the case of a majority policy, the state resulted in peer2 and peer3 being included in peer1 as well, simply because the majority wins. Also, let's consider the case where peer 1, peer 2, and peer 3 have different outputs, but the signatures were still reunited because the transaction was not rejected. What will happen is that when this transaction reaches the orderers, the orderers will reject its validity. This is because, despite passing the peer endorsement, the orderer endorsement will fail due to a lack of determinism between the three peers. Also, within the same database, there could be different slices of data for each \textbf{channel}. However, the concept of channel is something that should be delved deeper into more ahead.

Speaking about \textbf{CA's}(Certificate Central Authorities). This is a service that serves cryptographic material like private keys and x509 certificates, which within a \textbf{hyperledger fabric} network form a \textbf{MSP} (Membership Service Provider). \textbf{MSP} is a mechanism designed for providing a way to offer identities that could be trusted and recognized by the rest of the network. To achieve this, there is the usage of this cryptographic material provided by the CA's where a key pair is offered. One of these pairs, the private key, is used to sign transactions, and the other, which is public, gets shared among the entities that will be communicating with the owner of the private, giving them a way to verify that this signature is actually valid. \textbf{Hyperledger} lets the operator either rely on existing public CA's, use their CA implementation, or simply use a \textbf{hybrid} methodology. The main difference between the public and their implementation is that it's implementation already comes with a given file structure to make the configurations easier and also the aggregation of more responsibility to the operator, but that's something that is not the concern here, thereby it should not be extended.

Relatively to the \textbf{client} within the hyperledger fabric components sphere, there is a close relation in
how it operates with the chaincode. Like chaincode, both can be implemented with a certain number of
high-level programming languages such as \textbf{Go}. Both use an interface, attributed to their set of tasks in aim.
In the case of the \textbf{chaincode}, its interfaces are related to making interactions with the ledger, but in the case
of the client, its abstractions are focused on making calls against the peer by giving metadata related to
the \textbf{channel} and methods to be targeted. Thus, its more formal name is \textbf{Fabric SDK}, where there is the
opportunity to use \textbf{Node.js}, \textbf{Java}, and \textbf{Go}. Giving the opportunity to perform various functions, such as
creating \textbf{channels}, submitting \textbf{transactions}, querying the \textbf{ledger}, and managing the \textbf{chaincode}. Within
the \textbf{SDK} ,there are various subpackages aimed at serving different purposes, like: \textbf{gateway}, for establishing
connections to the network and provide access to \textbf{channels} and \textbf{contracts}. \textbf{Network}, which represents
a channel that a client can interact with. \textbf{Contract}, conceded for iteration with a given smart contract
deployed in a \textbf{channel}, used for submitting and evaluating transactions, and finally Wallet, responsible for
managing user identities and credentials for connecting to the \textbf{fabric network}. Further details should be
seeked within the proper documentation of \textbf{hyperledger fabric}.

Apart from what has been covered so far, there is the concept of \textbf{channel}. This concept is very important since it is the existing mechanism that allows you to aggregate a certain number of components, thereby creating a linkage between them and a precise section for them to exchange information. Within the documentation of \textbf{Hyperledger Fabric}, there is the reference to channel as a \textbf{group of friends}. This is because, exactly like a channel, you have multiple groups of friends on your social media, and their members are different depending on the task or the theme to be discussed within that group. Also, within those groups there is an exchange of information that is private and it's not shared with other group members, which is essentially the same thing as a channel. In a channel, you aggregate a certain number of components that could be part of multiple different organizations depending on what you want to be sharing within that channel, and also within this channel, there is the option to create a \textbf{private collection}. Despite the theme being the same, there are participants that may want to discuss information regarding this discussion within the group but just with a set of participants, especially like it would be with the private collection. Though the difference with a friend group is the nature of blockchain. This means that with a private collection, there is the option of creating a subset within the channel, making others able to see that you are communicating with each other but not what they are exchanging. In other terms, a given organization can be in multiple channels, with multiple different individuals in each, and it can also create multiple private conversations regarding the same theme, but just with a subset of participants in the same channel and exclusively of that precise channel. As an example, let's think of an organization that has the idea of creating a group of medicament's suppliers and other group of medical clinical reports. Obviously, in these examples, there will not be common participants because their nature is different. The organization may discuss within that group that it needs medicaments for X. Supposing they want to buy the cheapest one, they will request it from the enterprise Y. This enterprise, although offering it more cheaply, may not have the quantity to satisfy this need, making this organization forced to go to supplier Z to satisfy the rest of the demand. Maybe, for the benefit of this organization, prices of the product should not be known by the participants within this channel, which means that at least the monetary transactions within this channel should be done in private collections. However, the transactions of products could be done publicly, therefore making it easier for all of the participants to have the information publicly available that this organization is supplying itself from these two organizations, making others in the obligation of trying to bargain to also benefit from this channel. Also, the same applies to the clinical registry, even if it has a different logic or idea. There may be information that can be publicly exchanged between doctors, but there may also be communication that should only be sent within a more restricted set of individuals within that consortium. With channels and private collections, hyperledger obtains data confidentiality,selective data sharing and regulatory compliance \cite{private-collections}. 

\textbf {Endorsement Policies} are another big dinosaur in the room when speaking about \textbf{hyperledger fabric}. These are the rules under which the transactions will need to apply, making the transactions valid in case they succeed and invalid in case they fail to do so. In other terms, these are the policies that define which peers must sign (endorse) a transaction before it can be valid and aggregated to a block by an orderer. It specifies how many and which organizations must approve the transaction before it can be inserted into the ledger.

Piercing to the real use cases realm, there are several occurrences of hyperledger appliances. The sectors affected by this remarkable technology were: \textbf{Financial},\textbf{Healthcare},\textbf{Supply chain management},\textbf{Goverment and Public Sector}, and \textbf{Insurance}. Within the \textbf{Financial}, various banking consortiums were created for processing cross-border payments, managing the financial chain, and enabling transparent asset transfers \cite{finances-use-case}. In the \textbf{Healthcare} there are also a lot of occurrences of it's usage for managing electronic health records (EHR's), which is possible due to the channels and private collection capabilities, allowing sharing of patient data between healthcare providers \cite{healthcare-use-case}. In the \textbf{Supply chain management}, there are multiple occurrences for the usage of hyperledger fabric for tracking goods. By using this, enterprises were dotted of track food products from the farm until the shelf, reducing the time required to trace the origin of food products in the event of contamination while still offering privacy for sensitive business data using private collections \cite{supply-chain-use-case}. Also, when it comes to \textbf{Government and Public Sector}, it is well known that there are already present in the market implementations for the usage of such technology for managing digital identities, land registries, and transparent voting systems \cite{public-sector-use-case}. Finally, there is the usage for \textbf{Insurance}, where there are retained implementations that are meant to help claiming processing and reduce fraud. Thus, by levering this, the \textbf{Insurance} can collaborate in a shared ledger to verify these claims and manage shared data, reducing duplicated and fraudulent claims \cite{insurance-use-case}.

\paragraph{3.5.1.2- Ipfs}\mbox{}\\
When it comes to \textbf{IPFS}, it must be known that this does not represent a blockchain, though its concepts derive from a P2P perspective, which means there are relations between the two. It is a modular suite of protocols for organizing and transferring data \cite{ipfs}.

More concretely,\textbf{IPFS} is a tool that stands for Interplanetary File System, where it refers to an implementation that was meant to become the file system of the future.

Their aim is to serve as a decentralized file system where, to achieve that, what is required is having a distributed system that goes through an immense decentralized network of heterogeneous numbers of peers. This way, by adding a file or an image, there will be a unique identifier for that file. In the case of that file being a folder, it generates an identifier for himself and for the respective children's and stores it in an \textbf{DHT} (Distributed Hash Table) \cite{dht}; otherwise, it will simply create an identifier and store it in the same data structure mentioned before. With all of this done, anyone that wants to serve from that file only needs that the given peer is correctly configured and that everyone that has that identifier (which is called \textbf{CID}) have access to that file through a certain \textbf{IPFS} gateway, which can be public or private, depending on the situation you are in.

This solution can be interplanetary, because when the first request of a file in the network occurs, it may endure a lot of time, but after getting it, the file will be retrieved faster because it will be stored in cache in the most nearby peer. Thereby, if you plan to travel to another planet, it can be possible to share files in this network, which, depending on the range, may also include other planet files, but this has countless ways to occur. Or either by one of the peers from another planet changing its location to a location that is reached by our network or simply because the connection between those 2 planets is feasible. However, in order for this to work, there is still work related to making sure that the files are pinned; otherwise, they will get deleted later by garbage collection. This ensures that we do not have files permanently. If they are not that relevant, with time, they will simply disappear. To understand \textbf{IPFS} more in depth, there will be a vast number of concepts covered, so there is a vast understanding of this technology.

\textbf{Decentralized Data Storage} is one of the features of \textbf{IPFS}. It refers to the possibility of storing files across a network of nodes, where each node can also serve requests if they possess the file a given user is requesting, creating like this a distributed web of information. To achieve this, \textbf{IPFS} relies on a network of heterogeneous nodes, or p2p, where each user can retrieve files from any node that holds a copy of the requested data. Also, it gives a unique identifier to each file \textbf{CID}, making data immutable and verifiable. It uses \textbf{DHT} for the routing and discovering of nodes that store content. It breaks files into smaller pieces, where each piece is replicated across multiple nodes. It pins the data to make sure that the data remains on a specific node, guaranteeing its availability in case there are nodes dropping out, and by doing all of this, users that have a \textbf{CID} of a file can request it at any node thereby making the network effectively decentralized.

As it's second feature, there is the \textbf{content addressing} feature, which is it's capability to assign a CID to each content of IPFS. This identifier is unique to the content and does not depend on where the content is stored. This is due to the fact that the CID is generated by hashing the file, and the retrieval is done by using the \textbf{DHT} to find nodes and ask them if they have this hash value.

\textbf{Data Redundancy and Persistence} is another major characteristics within \textbf{IPFS}. Since it has the competence of persisting and making available data over time, even if there are failures with the network. Because of it's decentralized nature, it can offer redundancy by ensuring that multiple copies of a given file are stored across different nodes within a the network and it offers persistence because the file remains accessible and intact even if there are nodes going offline or data get's lost. To achieve this, it relies in a vast number of mechanisms. This mechanisms are \textbf{content replication},\textbf{data pinning},\textbf{fault tolerance} and \textbf{Data availability and load balancing}. \textbf{Content replication} is the mechanism where \text{IPFS} divides a file into multiple chunks and distributes them within the network between multiple nodes. It is with this practise that the data remains accessible even with offline nodes. \textbf{Data Pinning} however, is the instrument that this technology is doted that consist in marking a file as something that should be stored indefinitely, thereby preventing that a garbage collector cleans it. \textbf{Fault Tolerance} refers to having multiple nodes that may have the copy. This means that the system is resilient due to the fact that it makes data available if atleast one node that has this copy is not failing. Finally, there is \textbf{data Availability and load balancing}, meaning that by having multiple nodes where the copy got stored the access ends up distributed thought the nodes. Regarding this feature, there is a work that highlights the effectiveness of data redundancy in \textbf{IPFS} against a centralized solution \cite{ipfs-persistence}.

\textbf{Ipfs} also offers \textbf{Efficient Data Distribution}. This is possible because of its P2P architecture to distribute the data across the network, which gives high availability and fast retrieval. However, there are still discussions to improve even more the protocol by optimizing its node management and reducing redundant data delivery \cite{ipfs-optimization}.

Another important thing to mention is the \textbf{Censorship resistance} that it has. This is possible due to it's decentralized nature and immutable identifiers. In addition, there is a project that proves how \textbf{IPFS} techniques are effective against censorship practises such as: \textbf{blocking IPFS gateways}, where malicious actors can use techniques to block the point of access (gateway) from users using conventional practises(DNS blocking,IP address blocking,Deep Packet Inspection,Blocking HTTP headers, etc...).\textbf{Filter p2p traffic}, where bad actors filter information from the request or even the response with common practises (Deep Packet Inspection,Trafic Analaysis and Pattern Recognition,etc..).\textbf{Preventing IPFS boostrapping} that happens when the attacker blocks ipfs boostrap nodes which are nodes that new nodes connect initially to know others in the network, making unfeasible for new nodes to join. \textbf{Targeting Specific Content Hashes (CDI's)} where malicious actors don't retrieve certain CDI's because they may want to sensor that content. \textbf{Blocking IP addresses of Known Nodes}, which occurs when bad actors block ipfs nodes or even gateways. \textbf{Coercing or Compromising Gateway Operators}, which is a practise where someone makes the operator of a gateway censor determined \textbf{CDI's} and \textbf{Legal and Policy-based censorship} where the government decides prohibiting others to share certain content \cite{censhorship-combat}.

When it comes to the integration of such technology within the blockchain, there are various interesting projects. Firstly, there is the tokenization of storage, where the \textbf{Filecoin} \cite{filecoin} project gives the possibility to users to store their data for a fee that they should pay to a storage provider (or miner) in a decentralized environment. Secondly, there is the \textbf{NFTS} \cite{nfts}, another project that \textbf{IPFS} is very used for, which is for storing metadata that can later be used in a platform such as \textbf{Opensea} \cite{opensea} or even for storing the image itself. This metadata is a standard that is stored as \textbf{JSON} \cite{json} in the \textbf{IPFS} network, such that the \textbf{NFT}, by having the link to such \textbf{JSON} related to it on its smart contract, could have its characteristics mentioned in the \textbf{Opensea} store. Thirdly, there is the \textbf{Distributed Education Blockchain Network (D-SELI)}, which changes its SELI educational blockchain network by integrating IPFS to handle the storage and accessibility limitations. With this integration, materials are stored in a decentralized way, making them more accessible and robust against data loss or network failures \cite{ipfs-educational-platform}. Forthly, \textbf{Blockchain-Based Ride-Sharing Services Using IPFS}, which is a study that proposes a blockchain-based ride-sharing system that integrates IPFS. This aims to share data outside of the blockchain, answering issues around response time and scalability, which with \textbf{IPFS} and by sogregating the blockchain of the data storage, there can still be data integrity and traceability while improving storage and efficiency \cite{ipfs-sogregate-blockchain-from-storage}. Finally, \textbf{IPV4 and IPV6 for blockchain networks}, where there is an IPV6 blockchain network implemented alongside \textbf{IPFS}, which improves privacy, security, and scalability compared to IPV4 \cite{ipv6-blockchain-networks}.

\subsubsection{Microservices}
Within this section, there is information about the major high level technologies that were used in this precise research that are related to \textbf{microservices}. One of those technologies is almost a obligation for the current landscape of technology, there is one  which is a upgrade, one that is necessary in certain concrete situations and there is also a extension but all of them are valid. The one that is almost a obligation is docker, because currently in the tech landscape it is used in almost every system because of its isolation therefore faster for development in big teams. The upgrade is the kubernetes, which manages this \textbf{docker} images like they are resources ,making it complex in a certain way but also easier to manage in another after learning it. The necessary in certain concrete situations, in another hand is \textbf{MetalLB}, because it is meant to on premise implementations, which means that it is more suited for own infrastructure implementations which is a big help when it comes to healthcare due to their resistance into moving to the cloud. Finally the extension is \textbf{Istio}, because it is used to have more control over the network and its communications, but its usage its not mandatory, thereby in this project despite tested it is not used in any implementation.
After \textbf{Hyperledger}, this are the technologies that more represent how some implementations communicate, therefore is good to present what they are and how they work, so the reader can understand the architectures that will be presented further. 

\paragraph{3.5.2.1- Kubernetes}\mbox{}

\textbf{Kubernetes} stands as a open-source platform designed for \textbf{automating the deployment},\textbf{scaling}, \textbf{management of containerized applications},\textbf{management of resources}, \textbf{management of secrets} and \textbf{management of configurations}. Originally developed by \textbf{Google}, \textbf{kubernetes} has become a standard for managing containers being a premise to build cloud-native applications. By taking leverage of this, developers can abstract the infrastructure and focus more into the application logic instead of hardware and other operations side tasks. With this in mind, \textbf{kubernetes} today is centered as the pilar for building any application, even if it is on-premise(within a organization own infrastructure), due to the fact that currectly every organization seeks a \textbf{microservices architecture} which obligates businesses to build their own kubernetes clusters. This is because, microservices itself offers a lot of advantages for their business such as \textbf{high-availability} because of deployments, \textbf{better resource management} and \textbf{resilience} which are crucial for any organization that wishes to remain competitive. 

Relatively to it's key features, on the other hand there is the \textbf{Automated Container Deployment and Management} for automaticatically deploy and manage large number of containers across multiple nodes, \textbf{Scaling and LoadBalancing} for managing instances of the applications such that the resources are spared or for balance the requests per instance,\textbf{Self-Healing} for restarting a pod every time it crashes automatically,\textbf{Service Discovery} for discover a given service by using it's name that later on maps to a given ip, \textbf{Storage Orchestration} for manage and provide storage resources,\textbf{Declarative Configuration and Automation} for users define their application state in configuration files and finally \textbf{Rolling Updates and Rollbacks} which is a mechanisms for going back to the previous application version in case of error.

\textbf{Containerization} involves packaging an application and its dependencies (libraries, configuration files, and binaries) into a lightweight and portable box. This can be used in multiple environments without problems with dependencies. This is because this box, more often called a container and  sharing the same kernel with the main machine, segregates the dependencies and operative system, making it a huge allie in software engineering, and that's why containerization always has deep tights with \textbf{isolation}, \textbf{portability}, and \textbf{efficiency} (because it is lighter than the traditional VM's). The main technologies for containerization are \textbf{Docker}, but there are others such as \textbf{Podman}, \textbf{CRI-O}, and \textbf{LXC}. To manage this, there are existing abstractions. The most well known is \textbf{Kubernetes}, and the main reason for using it is that it treats these containers as resources, it has abstractions to deploy them, it manages secrets to use on them, it helps in creating horizontal and vertical scaling depending on metrics, it has mechanisms to go back in case the deployment goes wrong, and it ensures high availability by resetting containers every time they crash, though there are other ways.


\paragraph{3.5.2.2- Docker}\mbox{}\\
\textbf{Docker}, on the other hand, is a  technology that could be abstracted by the previously mentioned \textbf{Kubernetes} and is said to be a tool to build applications in a different paradigm than before. To contextualize, in the past it was something normal to install dependencies directly in your local machine, which created some burden since you may have had different projects with different dependencies versions. Also, the application would mess with the local machine configurations because it was directly using it's resources, having more probability of making adjustments that would compromise other projects by having different configurations. By acknowledging these limitations, Docker came to light because it was conceded to automate deployment, scaling, and management of these referenced applications using a new concept called \textbf{containerization}.

Before containers, there was a concept called \textbf{virtualization}. This is still widely used, but it became less with the introduction of containers. This is because, despite being very powerful, \textbf{virtualization} is more heavy than a container, though offering the same characteristics.

The main advantages, on the other hand, for this tool are \textbf{isolation} from the operative system, the possibility to run \textbf{more instances} compared to VM's, \textbf{portability}, \textbf{more collaboration} within the team, \textbf{version control}, \textbf{streamline process} capability, and the capability of \textbf{scaling} if used along side kubernetes. Thereby, it's main features are: \textbf{Lightweight Virtualization}, \textbf{Containerization}, \textbf{Portability}, \textbf{Isolation}, \textbf{Version Control and Reusability}, and \textbf{Scalability and Efficiency}.

\textbf{Containerization},like mentioned before is a technology in software engineering to package all dependencies and software all together, creating a environment that shares the kernel with the host but isolates the rest of the resources, while being a lot more light than VM's. Because of this,the application runs in any kind of environment, since this creates isolation. By the cause of such, the main characteristics associated with \textbf{docker} are \textbf{Isolation}, since processes,memory and file systems are totally segregated, such that multiple containers can run without interference with each other. \textbf{Portability}, because it packages everything in a single unit. \textbf{Efficiency}, due to the fact that is very lightweight and \textbf{Scalability} because it can be scaled up or down easily and there are orchestration tools that make this even more easier such as the previous mentioned \textbf{Kubernetes}.

This tool is core for \textbf{CI/CD Pipelines}. \textbf{Continuous Integration} (CI) is a pipeline that aims to create a flow for code, build, and test applications, while \textbf{Continuous Development} (CD) is a pipeline for release, deploy, operate and monitor applications. To achieve this, \textbf{docker} is the tool that is used to build images, to release, deploy, and to monitor. These practices are very important because they speed up the development, thereby making Docker very important from this perspective. Also, another usage of Docker is within the \textbf{kubernetes} realm, since this is the containerization technology that is often more used. Kubernetes requires Docker because it manages containers, where Docker fits religiously.

\paragraph{3.5.2.3-MetalLB}\mbox{}\\
\textbf{On-premise cluster} is an advanced concept related to setting a network of computers within your own infrastructure. In the case of this project, the reference to this is related to a network composed of \textbf{kubernetes} nodes. Because of this, it is imperative to mention \textbf{MetalLB}, which is a \textbf{load balancer} conceded just for \textbf{on premise Kubernetes clusters}.

When it comes to \textbf{kubernetes}, usually the tendency is to relate it to the cloud. However, as the time progresses, components become more powerful and cheaper, and more and more, the consideration of moving to the cloud or not becomes more a question of cost and responsibility. This is because, though the power of components is becoming stronger, the complexity of hardware is sometimes increased, making organizations choose between being themselves responsible for the maintainability of the infrastructure or simply giving it away to those that are more specialized in the subject, which may or may not reduce costs for them because not only do they have the common infrastructure costs but also the burden of finding and maintaining qualified personnel for this type of job \cite{onpremice-cloud}.

\textbf{MetalLB} comes into play when it comes to those more audacious to have all the responsibility. As a strong \textbf{Load Balancer} for \textbf{on premise} networks, it stands in this project as a marvelous tool.
for achieving load balancing, which is a concept where the requests are distributed through multiple instances of the same application, balancing this way the load that each may have. Cloud distributors have their built-in support for such, while on-premise networks do not, which makes \textbf{MetalLB} a must for setting such capability in a non-cloud environment.

With this in mind, with features such as \textbf{Load Balancing for Bare-Metal Kubernetes}, \textbf{Layer 2 and Layer 3 Modes (BGP Mode)}, \textbf{Address Pool Management}, \textbf{Seamless Integration with Kubernetes}, and \textbf{High Availability}, \textbf{Metalb} is indeed a genesis block when it comes to mirror cloud environments locally, therefore this section will be covering it at its best.

To put it more clearly, the need for \textbf{MetalLB} in Bare-Metal Kubernetes cluster has to do with several reasons, such as: \textbf{Lack of Cloud Load Balancer Services}, \textbf{Direct Integration with Bare-Metal Network Infrastructure}, \textbf{Ease of Configuration and Flexibility}, \textbf{Avoiding the Need for External Hardware Load Balancers}, \textbf{Support for LoadBalancer Type Services}, and \textbf{Scalability and High Availability}. In regards to the \textbf{Lack of Cloud Load Balancer Services}, this means that on-premise there is no load balancer out of the box like in a cloud environment. Thus, in the cloud there is no need at all to implement your own, while in your own infrastructure there must be a settled one; otherwise, load balancing will be a feature out of scope. Direct Integration with \textbf{Bare-Metal Network Infrastructure} means that \textbf{MetalLB} can integrate this functionally directly with the existing infrastructure. This is done either with IP advertising in \textbf{Layer 2 mode (ARP)} or with \textbf{Layer 3 (BGP) mode}. \textbf{Ease of Configuration and Flexibility} because \textbf{metallb} it's easy and flexible to configure, like it will be mentioned in future use cases. \textbf{Avoiding the need for external hardware load balancers} it's also another good characteristic, which means that, while other organizations resort to the usage of expensive hardware for load balancing or complex software-defined networking, \textbf{Metallb} offers a software-based and yet native approach to manage the necessary operations to come up with this load balancing \cite{hardware-vs-on-premise}. \textbf{Support of LoadBalancer type services} is another big factor, since without a load balancer this is not possible, where \textbf{metallb} comes into action. Finally, \textbf{Scalability and High Availability}, since it can scale with the number of nodes within the cluster, distributing service traffic evenly across multiple nodes.

Within a more deeper analysis of \textbf{l2} mode, it must be known that it works under a local area network (LAN). This is the simpler configuration mode supported by \textbf{metalLB} and works with the leverage of both \textbf{ARP} and \textbf{NDP} to advertise the presence of a service IP within the local network. With this implementation, what happens is that a ip address is generated from a pre-defined pool of ip addresses and then, recurring to ARP, it announces a given IP that then is requested to the network and exchanged by a \textbf{MAC} address.

On regards to \textbf{l3}, on the other hand, it works using external routers. This makes it more suitable for larger clusters or envronments where complex routing and multi-subnet configurations are required. Also, it offers more advanced capabilities such as network policies and fault tolerance. It works by making sure that a given external router is compatible with the speakers, it communicates with them to check the ips to be advertising and makes those available outside of the cluster where any request will pass throught them and get redirect to the correct node in question.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/tools/metallb/bgp-vs-l2.png} % Change this to your image file
    \caption{Layer 2 vs Layer 3 Mode}
    \label{fig:sample-image} 
\end{figure}

\paragraph{3.5.2.4- Istio}\mbox{}\\
In this thesis, \textbf{Istio} appears more like an extension and not concretely as something that was of extreme usage. However, because it was used to determine whether each measure is useful and also because it is very different from other concepts, explanations over what it is are required because, although not extensively used, it is mentioned in the thesis.

\textbf{Microservices} is an architecture of exalted power. However, what comes with great power also requires great responsibility. Therefore, conducting the management of the communications between services is not a straightforward task. By virtue of such, the more the application expands, the more work there is around this matter, making \textbf{DevOps} teams struggle when it comes to make optimization and manage this within the application realm.

By means of this, \textbf{Istio} surges as alleviation of such. At the hand of its capabilities, it serves as a \textbf{service mesh}, which is nothing more than an abstraction created around the application for providing \textbf{Traffic Management}, \textbf{security}, \textbf{Load Balancing}, and \textbf{monitoring}.

As mentioned before, cloud-native architectures and microservices are becoming something permanent. Such that managing it is a challenge. Thus, using a \textbf{Service mesh} could potential be something meaningful because it works as an abstraction layer that handles the dynamic networking and communication between microservices. With this brief introduction in mind, thinking in the most important aspects for which this is needed is something intriguing. Firstly, \textbf{enhances microservice communication and networking} is one of the aspects for including istio. This is because, a service mesh provides service-centric networking, thereby oferring features that are not native to kubernetes. This is helpful for managing services communications with capabilities such as load balancing,retries,timeouts and circuit breaking \cite{service-mesh-enhance-communications}. In second, despite current implementations create overhead in terms of resources, due to the proxys and the new side containers, there are service mesh implementations that actually aim to \textbf{improve performance and resource efficiency}. This can be done with new architectures like \textbf{FlatProxy}, which reduce performance bottlenecks and improve resource utilization \cite{service-mesh-flat-proxy}. As a third motive, there is the aspect of \textbf{Observability and Monitoring}. This as to do with the fact that a service mesh collects metrics,traces and logs for each service interaction, providing insights about the behavior of the microservices enabling performance bottlenecks spotting, understading of service dependencies and improving system debugging \cite{service-mesh-monitoring}. In forth, it should be known that it also facilitates traffic management through fine-grained control policies, such as traffic splitting,canary releases and fail-over strategies. However, there are studies affirming that it creates overheads such as 269\% latency and 163\% CPU usage, emphasizing the need for optimization \cite{service-mesh-traffic}. When it comes to the final, the \textbf{Security and Resilience} is another aspect that serves as a benefit to use it since a service mesh usually offers mutual tls out of the box with certificate rotation \cite{service-mesh-security}. 

Since this works out of the box like an abstraction over the traffic, it becomes clear that observability and monitoring over it would be feasible. What can be found here is \textbf{Metrics collection} , \textbf{distributed tracing} , \textbf{logging} , and \textbf{visualization capabilities} . Relatively to \textbf{Metrics collection} , they cover an amplified range of matters, covering HTTP and TCP traffic. This data is usually in time-series format, where Prometheus comes to play to be its widely used collector. This way, Prometheus usually collects the data, and Grafana presents it to someone. Some of the metrics that can be found are something like \textbf{istio\_requests\_total} for the total number of requests that a service received,\textbf{istio\_request\_duration\_seconds} for measuring latency in seconds, \textbf{istio\_request\_size} for checking the size of the requests received,\textbf{istio\_response\_size} for checking the size of the requests sent and \textbf{istio\_tcp\_connection\_opened\_total} for the total number of TCP connections opened. For \textbf{distributed tracing} , there are integrations with tracing tools such as \textbf{Jaeger} , \textbf{Zipkin} , and \textbf{OpenTelemetry} , which allow us to understand the existing flow of communications between services, helping in debugging, bottlenecks, and error detection. Also, within the \textbf{logging} realm, Istio offers a generation of logs for all the service requests and responses, providing detailed information about such, which is imperative for things like troubleshoot detection, auditing of service interactions, and paths and headers monitoring. For visualization capabilities, on the other hand, Istio also offers good options, such as the integration with \textbf{grafana} and \textbf{kiali} . For both, existing dashboards are already at the disposition of everyone, which means that little configuration must be done, though Grafana is more suited for all and Kali is more custom-made to service meshes, presenting more capabilities out of the box.

The key component of Istio is the sidecar container based on \textbf{envoy}. This is what is deployed along with each service within the mesh, offering security, traffic management, observability, and resilience for microservices while ensuring compliance with policies, traffic splitting, security enforcement, and telemetry collection. It is a high-performance edge and service proxy designed for cloud-native applications and has become the default data plane for Istio.

Istio is also doted with multiple features to increase reliability. These features are very well known in the distributed systems context and tend to help ensure that microservices can handle graceful failures, prevent cascading issues, and provide robustness in complex service interactions. These services are: \textbf{Circuit Breaking}, which is a pattern used to prevent a system from repeatedly making requests to the same failing system. When a given number of requests fail, this mechanism blocks the service for a given period of time, enabling him to recover and avoid it's exhaustion; \textbf{Retries} are another mechanism, but it stands for retrying the request in case it fails before returning an error to the client, and \textbf{timeouts} for a maximum time that a service should wait for a response from another service. In case the request exceeds this timeout, the request gets terminated \cite{circuit-break-and-timeouts}\cite{circuit-breaking-and-retries-2}\cite{circuit-breaks-and-retries}.


\subsubsection{Analytics}
This chapter centers it's focus in Analysis. When there are ideas about a given system, usually people don't think right ahead on this matters. Despite the possibility of creating systems without such capabilities, it will become clear further in time that it will be painful and more like a burden if there are none, this is because, the more complex the system gets the more there is a need to auditing and this is only possible if there are mechanisms prepared for the analysis of the network. In other words, despite not directly mandatory, it is almost a obligation to make our infrastructure doted of such, because by having good design over the analysis, auditing becomes more easier and there is the possibility of knowing where to do better by spotting, for example, bottlenecks but also where in the system something is failing (debug). Also, there is a need of having tools that simulate real behavior from users so that the responsible knows the limits of the given System. With this in mind, in this section, it will be covered all of the tools that in conjunction are used to deliver this capabilities that were just spoken. Firstly there will be a discussion about \textbf{Caliper} which is a framework to benchmarking blockchain networks, such as \textbf{hyperledger fabric} in our research projects. Secondly, there is \textbf{prometheus} overview which is a time-series data supplier. Thirdly, there will be information about \textbf{cAdvisor} which retrieves measures from machines and finally there will be a quick representation of \textbf{grafana}, because altought not used in the final implementation, it remains crucial for benchmarking since it has more visualisation capabilities for result comparison, which makes it more feasible than the ones built inside of the project.

\paragraph{3.5.3.1- Caliper}\mbox{}\\
In what respects the domain of \textbf{Benchmarking}. It is well familiar that it represents the \textbf{measuring} and \textbf{comparing of performance}. As the need of metrics arises during a project, there will be space for such concept and within this thesis this could not be different.

In prospect of the usage of such concept, within this project,there will be justification of results. Thereby, the need to use a tool such as \textbf{Caliper} and the requirement for further clarification of it, which will be stated in this category.

\textbf{Caliper} \cite{HyperledgerCaliper} is a open-source multi-function benchmarking tool. It is part of the hyperledger project and allows it's clients to evaluate various aspects of their blockchain implementations. This aspects could be \textbf{transaction throughput},\textbf{transaction latency},\textbf{resource utilization} and even \textbf{scalability}. Thought it's modular architecture, it is possible to implement a vast number of custom connectors, enable the developer to connect in various ways to multiple blockchains. Also, there is by standard connectors for a lot of network tastes such as \textbf{Hyperledger Fabric},\textbf{Ethereum},\textbf{Hyperledger Besu} \cite{HyperledgerBesu} and \textbf{FISCO BCOS} \cite{FISCOBCOS}. This flexbility allows those that have power over it to compare different solutions. Key features of it are: \textbf{Benchmarking Across Multiple Blockchain Platforms},\textbf{Customizable Performance Tests},\textbf{Detailed Performance Metrics},\textbf{Modular and Extensible Architecture},\textbf{Automated Benchmarking} and \textbf{Visualization and Reporting}.

Relatively to the key metrics in the hyperledger caliper, there is the \textbf{Throughput}, which is a measure of how many transactions a network can handle per unit time. \textbf{TPS (Transactions per second)}. \textbf{Latency}, which is the time taken for a transaction to be confirmed and added to the ledger after submission, \textbf{sucess rate}, representing the percentage of submitted transactions that were included, \textbf{transaction response time} is the time that a response takes to come from the network, including both validation and commit phases. \textbf{resource utilization} for measuring the usage of CPU, memory, disk I/O, and network usage across different nodes, \textbf{read/write operations} to measure the number of read and write operations performed during the execution of transactions; \textbf{commit time} to the time required for a transaction to be committed to the ledger after being validated; \textbf{block creation time} for the time it takes to create a block from validated transactions; \textbf{transaction ordering time} to measure the time taken to order transactions before they are batched into a block; and \textbf{failure analysis} to analyze the reasons for transaction failures like invalid input, lack of consensus, or even resource constraints.

In order to come up with a benchmark, there are several workflow steps to be taken. Firstly, it should be defined as a benchmark configuration within the \textbf{benchmark.yaml} file, which includes network configurations, test scenarios, and workload parameters. Secondly, create the necessary configurations in the \textbf{network.yaml} and test it to see if it does work against the desired network. Thirdly, execute the workload module, which is the transactions that could be either of read type or write type. Fourthly, submit the transactions using the fabric adapter, connecting to the more suited node to ensure that transactions are validated, ordered, and committed to the ledger. Fifthly, the transactions get submitted and go through a validation phase where endorsement policies are checked. In case everything goes well, the transactions will be batched into blocks by the orderer and distributed among all the peers, giving a positive or negative response depending on whether it was successful or a failure. Sixth, the monitor module collects all of this data on system resource utilization, such as CPU, memory, disk I/O, and network usage in all participating nodes. Sept., there will be the reporting generation that the user must analyze. Finally, there is a result aggregation, and a final report is generated, which can be later used to take action over possible bottlenecks or misconfigurations.

\paragraph{3.5.3.2- Prometheus}\mbox{}\\
\textbf{Prometheus} is an open-source \textbf{monitoring} and \textbf{alerting tool} designed for recording real-time metrics. At the essence of this thesis, there will be instances where \textbf{Prometheus} is mentioned. This is because it is very useful to gather data from various places at the same time, making it easier to grab the data at a single point of failure. In other terms, it is a tool based on \textbf{time-series} and it remounts to the collection of metrics from various systems and services, making it very suited for this kind of project. Also, it provides various useful features such as \textbf{Time-Series Data Model},\textbf{Pull-based model},\textbf{PromQL},\textbf{Alerting},\textbf{Service Discovery},\textbf{Multi-Dimensional Data},\textbf{Efficient Storage} and \textbf{Dashboards and Visualization}.


\paragraph{3.5.3.3- Cadvisor}\mbox{}\\

As mentioned before, containerization is a concept for package applications. It creates lightweight,standalone  and executable applications, while giving to them all the necessary dependencies to work in a portable,scalable and isolated way. By the effect of such, containers have gained an extreme importance in the current sphere of software engineering,making them of a big magnitude of importance to every single application that needs to be done nowadays.

With this in mind, and within the curious landscape of congregating measures, there is \textbf{Cadvisor} as a very useful tool that was built by Google. It has enormous advantages when it comes to the collection of data from insights about containers. By collecting and exposing metrics such as \textbf{CPU}, \textbf{memory}, \textbf{network}, and \textbf{disk usage}, it helps operators to monitor \textbf{resource consumption} of containerized workloads. Also, it works like a glove when combined with \textbf{Docker}, making it a valuable resource for providing a complete observability of containers. It's features range from \textbf{Container Resource Monitoring},\textbf{Per-Container Isolation},\textbf{Historical Performance Analysis},\textbf{Container Health Metrics},\textbf{Integration with Monitoring Tools},\textbf{Low Overhead} to \textbf{Kubernetes Integration}, making it imperative to be delved into deep terms in this section.

In terms of integration, it does integrate with various platforms and tools, which provide the previous mentioned features. The most well known platforms are \textbf{Kubernetes}, \textbf{prometheus} and \textbf{influxdb}. Also, it's use cases are enormous like performance analysis and bottleneck detection, resource allocation optimization and application behavior analysis.

\textbf{Cadvisor}, within the advantages, can automatically detect multiple containers while supporting multiple container runtimes like \textbf{Docker} and \textbf{Containerd}. Second, it is very light and has the capability of giving info in real-time, which means that it not only does not represent an overhead for the network but also projects real-time metrics for further usage. In third, it was an easy integration with \textbf{Kubernetes}. This is because cadvisor is already contained within the kubelet module, which is present in every single instance of a node within the cluster and kubernetes uses it as well for pod autoscaling (HPA) to automatically scale applications based on observed resource usage. As a third advantage, it is very easy to implement with the most used tools available on the market, like \textbf{kubernetes}, \textbf{prometheus}, and \textbf{influxdb}, as mentioned before. As a fourth advantage, there is also the versability and flexibility that it offers since it supports multiple container runtimes and also enables custom metrics that could be exposed inside of the proper containers. Fifth, it also has a built-in visualization and user interface UI, which, despite being very simple, could potentially be very interesting from the operator's point of view, making them empowered with fast troubleshooting and analysis. Also in sixth, this is the standard technology to make this monitoring possible, and it could potentially extend to more advanced monitoring solutions such as long-term storage, anomaly detection, and predictive analytics. In seventh, there is the cost-efficiency and resource optimization, meaning that it helps the administrator to make optimizations in the network by alerting bottlenecks and describing the communication behavior between the services. Finally, there is the community support and open-source nature as another advantage, since it creates a sense of active development and community support, thereby having active updates, bug fixes, and new functionalities, while also making it possible to have custom functionalities that could be added by the operator or by someone that wants to help inside of that community.


\paragraph{3.5.3.4- Grafana}\mbox{}\\
For showing data within the \textbf{Benchmarking} landscape, there is the honored mention of \textbf{Grafana}. This is a open-source platform for \textbf{monitoring},\textbf{visualizing} and \textbf{analyzing} real-time data. Thus, alongside \textbf{Prometheus}, it's useful to gather data that was collected during tests of performance, but this will be covered better in a future discussion. With this in mind, it is valuable to debate such tool into this segment.

When abording the data sources and integrations of Grafana, it should be known that it supports multiple data sources, making it easier to create visualizations, dashboards, and even alerts. The supported data sources are \textbf{prometheus}, which was covered already in the previous section; \textbf{influxDB}, which is another time-series database used for monitoring and alerting; \textbf{Elasticsearch}, which is a distributed search and analysis engine usually used for logs and unstructured data, making it worth when searching and visualizing logs and event data; \textbf{Graphite}, a monitoring tool used for storing and visualizing time-series data for analyzing system performance metrics; \textbf{AWS Cloud Watch}, for monitoring resources and applications that belong to AWS; \textbf{Google cloud monitoring}, providing monitoring, logging, and diagnostics for applications hosted on Google Cloud; \textbf{Azure monitor} for monitoring Azure cloud resources; \textbf{SQL Database}, allowing querying and visualization of relational data like those from MySQL, Postgresql, and MS SQL; \textbf{Loki} for aggregating logs, enabling log collection, search, and visualization in Grafana alongside metrics; Finally \textbf{OpenTSDB} for scalable and distributed time-series data. With all these integrations, Grafana stands firm as a universal platform for showing data, making it feasible for this project.

As it's core, Grafana supports multiple and different types of dashboards. A dashboard is a collection of multiple panels that together make a view, enabling users to visualize data from various sources. Panels in Grafana have 5 types, like \textbf{Graph Panel}, which is a default panel type used for KPI's and critical metrics; \textbf{Stat Panel}, to show a single value for scenarios such as KPI's and critical metrics; \textbf{Table Panel}, to display metrics in a tabular format, ideal for detailed views; \textbf{Heatmap Panel}, for visualizing high-density data, for showing distributions or patterns; \textbf{Logs Panel} for allowing logs from sources like Loki and Elasticsearch.

\subsubsection{Web Development}
To build any kind of application, there is a need for standard suitable tools. Since the projects involve creating multiple microservices, there is a huge dispersion in terms of frameworks. This is because, some technologies are better for certain tasks and others are more suited for other kind of use cases. There are cases where within the project it can be spotted 2 to 3 different programming languages and with that also different frameworks. This is because some are better for certain \textbf{API's} (ex: Kubernetes API) and others are better for faster build and development in containerized environments (ex: quarkus) which is fine in one hand, if considered that microservices usually means heterogeneous services but it's a struggle in terms of complexity, because it requires a high skilled set of individuals within the team, all to handle that context switch.
    With this in mind, here resides the most important concepts, specially presented for the web development realm. There are plenty of other technologies/concepts but these are the ones that more satisfy the condition of understandability.  Firstly there is React, to create UI's in a more reusable way. Secondly, Keycloak for managing permissions to communication between services, which is very important when it comes to fetching information which it will be the case. Thirdly, quarkus for serving some information within the project. Forthly graphql, which is a protocol to serve \textbf{HTTP} request. Fifth \textbf{REST}, which is a alternative to graphql but more used in the web development due to it's \textbf{CRUD} capabilities and finaly there is gRPC which is more suitable for more micro and high performing tasks such as side container tasks as an example.
Despite all frameworks and protocols differences, they are all essential to build some ideas that were gathered in this project, giving perspectives of robust and well design solutions where each has it's given task to serve.

\paragraph{3.5.4.1- React}\mbox{}\\
If further examination around the questions under research is performed, particularly the second, then it may be expected that there is a UI evolved. Not as a obligation, but more like something that could eventually occur. In the case of this project, there will be mentions around creating one for management purposes of a infrastructure, but more will be revealed as the use case cases come closer. Thus, explanation of the given tool that achieves this must be taken.

\textbf{React} is a tool to build web applications, centered in the paradigm of reusing multiple components, components this, that are modular and can be used in multiple places of the application. Instead of using the traditional way of creating a HTML code for each page, with this technology there is the option of for example using a piece of code that represents a top bar and apply this same top bar in multiple pages, having this way to write less code and also make the application easier to develop. Also, it is easier to make components render in the desired way when data changes occur. 

\paragraph{3.5.4.2- Keycloak}\mbox{}\\
When it comes to management of access to services, \textbf{Keycloak} surges as a valuable asset. It is an open-source \textbf{identity and access management (IAM)} solution aimed at modern applications, services, and APIs. Offers \textbf{authentication},\textbf{authorization} and \textbf{identity federation services}. In this section, there is a deep dive into microservices, and having a technology that could create means for authentication when dealing with a determined number of services that get exposed at a global level is important. With this in mind, let's dive into the main capabilities of such a valuable arsenal.

Like previously spoken, \textbf{Keycloak} is an \textbf{identity and access management tool} (IAM). Which means that it was processes to manage digital identities and control access to resources within an organization. To accomplish this, security policies are enforced, authentication and authorization are secured, and compliance is also expected because there is a management of who has access to what resources. The key functions of IAM are \textbf{authentication}, \textbf{authorization}, \textbf{user management}, \textbf{single sign-on (SSO)}, which stands for allowing users to log in only once and keep the access, and \textbf{federation} to authenticate users across multiple identify providers.

As a key security feature, \textbf{Keycloak} has \textbf{RBAC (Role-Based Access Control),} which enables administrators to define and manage user permissions based on roles. This mechanism enables that only authorized users have access to specific resources, providing a fine-grained approach to securing applications and services. To implement this, \textbf{keycloak} must use it's \textbf{Real Roles} and \textbf{Client Roles}, where there is the need to assign permissions over both global (real) and application-specific levels (client).

Keycloak also supports \textbf{OAuth} 2.0 and \textbf{OpenID Connect} (OIDC), which are both frameworks used for authorization and authentication in modern applications. \textbf{OAuth} is an authorization framework that enables applications to obtain limited access to user accounts on an \textbf{HTTP} service. \textbf{OIDC}, on the other hand, is something that enables clients to verify the identity of the user and obtain profile basic information. In other terms, \textbf{OAuth} 2.0 is used for authorization, while \textbf{OIDC} is used for authentication\cite{oauth2.0} \cite{openid} \cite{jwt}.

Speaking more concretely about authentication security, it must be known that Keycloak supports \textbf{Multi-factor authentication (MFA)}, requiring users to provide additional verification methods beyond a username and a password. With such capabilities, the user is usually forced to combine multiple practices for authentication. As an example, there could be combinations such as password and pin, a one-time password generated by authentication and a code received via SMS, and even fingerprint and facial recognition biometrics; basically, the usages are endless. However, for keycloak the ones supported are \textbf{time-based one-time password (TOPTP)} which can be used with authenticators such as Google authenticator or Authy, \textbf{webauthn} which enables users to authenticate themselves with hardware, such as a yubikey, \textbf{SMS OTP} which is a code that gets's received in the phone, and \textbf{email OTP} which is receiving codes via email \cite{MFA}.

\paragraph{3.5.4.3- Quarkus}\mbox{}\\
\textbf{Quarkus} is an open-source Java framework, designed specifically to \textbf{optimize} Java applications for \textbf{Kubernetes}, \textbf{containerized environments}, and \textbf{cloud native development}. It's usage is still gaining presence due to its premature environment, but it does show to be valuable when it comes to building and starting up containers, and that is the reason why it is called the cloud native Java solution. By working seamlessly with \textbf{GraalVM}, which is used to compile Java apps to native executables, it reduces the \textbf{start-up time} and \textbf{memory consumption}. Also, there is also included the \textbf{live reloading} feature, which, despite being useful because there is sometimes the need to change the application within the cluster, may be a bit worthless because of the existence of technologies such as \textbf{devspace} since it allows programming within the cluster and at the same time runs tests like it would be done in a local machine. But with this in mind, it must be said that this tool does improve productivity, and it was a powerful tool for the arsenal of this project \cite{quarkus} \cite{quarkus2}.

\paragraph{3.5.4.4- Graphql}\mbox{}\\
Within the landscape of web development, there is always the urgency of adopting an approach for the implementation of APIs. This is of extreme importance and can be difficult due to all of the aspects that the given tasks may take. Ranging from a vast number of characteristics, the ones that should be more taken into account when choosing which methodology to use are \textbf{Data Model} (if it should be RPC-based, resource-based, or query-based), \textbf{data format}, \textbf{protocol of transportation}, \textbf{performance}, \textbf{if it should have real-time data supply}, and \textbf{typing}. With this in mind, within this section there will be discussed \textbf{GraphQL}.

\textbf{Graphql} is a mid-level, high-performance approach for designing API's. It is \textbf{query-based}, supports \textbf{JSON}, \textbf{has a single endpoint for making the queries}, \textbf{runs in http}, \textbf{supports real-time with subscriptions}, and \textbf{it is strongly typed}. It was invented by Google, and it is used usually for \textbf{complex queryies} and \textbf{dynamic data requirements} use cases like \textbf{Social media apps}, \textbf{dashboards}, and even \textbf{mobile apps}. The reason for exceeding in this area is because while in \textbf{REST}, there is the request of all parameters of a model, in this approach you \textbf{only request what you want from a model}, therefore with this approach there is better performance overall in the client side and more overhead in terms of CPU for only returning what was requested by query. As an example, let's think of a model \textbf{Person}, where the parameters are: \textbf{name} and \textbf{age}. With this approach and by using queries, there is the possibility to only request the name and leave the age behind. At first glance, it appears that this has not much relevance, but when it comes to thousands of records and full of relationships, this approach becomes meaningful, and if mobile development is considered, it becomes even more so because of the even more limited resources when compared to a computer. Thereby, the usage of such is indeed useful because, although the resources within mobile are becoming hugely better, there are still a lot of people that cannot afford to have good resources, which could potentially exclude them from using their favorite applications, be it either \textbf{Facebook}(the one that created this approach) or \textbf{X} \cite{graphql}\cite{graphql2}\cite{graphql3}.

\paragraph{3.5.4.5- Rest}\mbox{}\\
When it comes to the most widely used approach for designing API's, \textbf{REST} is the way that is more present in every single organization. Though less performant in a lot of occasions, it is still the most standardized one, thereby the most used and more accepted. It is \textbf{resource-based}, \textbf{has multiple endpoints as a source of data}, \textbf{uses HTTP}, and is \textbf{typically loosely typed}. The best use cases for this type of way are \textbf{Simple CRUD operations} and \textbf{resource-centric API's} which, more concretely, correspond to \textbf{web services} and \textbf{public API's} applications \cite{rest}.


\paragraph{3.5.4.6- gRPC}\mbox{}\\
Delving more into \textbf{Microservices} tools, the \textbf{gRPC} approach can be seen as a valuable arsenal for that sort of applications. This is because this way of creating APIs is the most efficient because it's presented at a at a lower level, which can be a huge helper for extending networking communications between services. Coming from Google, this is an RPC implementation. The \textbf{g} stands for \textbf{google} and \textbf{RPC} for the remote procedure call. Usually seen as the modern implementation of \textbf{java RMI}, it works with \textbf{protocol buffers} which is more performant than \textbf{json}, it has a \textbf{single endpoint with method-based calls}, \textbf{uses http2}, it's \textbf{high-performant} and \textbf{lightweight}, supports \textbf{real-time share of data with bidirectional streaming} and is \textbf{strongly typed}. Also, the best use cases for this sort of approach are \textbf{high-performance}, \textbf{low-latency communication}, and \textbf{microservices}, with common applications being \textbf{microservices}, \textbf{real-time streaming}, \textbf{internal API's}, and \textbf{extension of traditional application communications with more modern ones}.
This approach exceeds this project since it is very good for extending communication capabilities and is also the protocol used to build hyper ledger fabric \cite{grpc}\cite{grpc2}.

